{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import NLPlib as nlp\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import HTMLParser\n",
    "\n",
    "import StringIO\n",
    "import string\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Pre-process, tokenize and tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CSV Format:\n",
    "\n",
    "0. the polarity of the tweet (0 = negative emotion, 4 = positive emotion)\n",
    "1. the id of the tweet (e.g., 2087)\n",
    "2. the date of the tweet (e.g., Sat May 16 23:58:44 UTC 2009)\n",
    "3. the query (e.g., lyx). If there is no query, then this value is NO QUERY. \n",
    "4. the user that tweeted (e.g., robotickilldozr)\n",
    "5. the text of the tweet (e.g., Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GID = 4\n",
    "class_zero_data = [GID * 5500, GID * 5500 + 5] # (GID + 1) * 5500 - 1]\n",
    "class_four_data = [GID * 5500 + 800000, GID * 5500 + 800000 + 5] # (GID + 1) * 5500 - 1 + 800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_buf = StringIO.StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Natalia_Bella not much to buy now Woolworth closed down \n",
      "Kill me please -.- ...Oh crap school tommorow \n",
      "@chriskeating re the labour general secretary meeting with Labour PM's aide - I posted the very same on facebook. BBC gone downhill \n",
      "Whole day of homework ahead of name \n",
      "hamlet...romeo n juliet...radio:ACTIVE live at Wembley...McFly tour DVD's too money to me \n",
      "staying at home like the good girls do \n",
      "Morning world. It's a beautiful day  Here's hoping for some pathetic fallacy\n",
      "Oh, how I wish @johncmayer  would say hello to me on a tweet.  That man is a God in my eyes...and ugh, the body....okay I'm done \n",
      "Just woke up. Eating sandwiches and drinking coffee  Oh yeaa..\n",
      "@JimLundy  we have made it very easy for them to catch up  http://bit.ly/5TUpg\n"
     ]
    }
   ],
   "source": [
    "with open('training.1600000.processed.noemoticon.csv', 'rb') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    \n",
    "    for row in itertools.islice(reader, *class_zero_data):\n",
    "        print row[5]\n",
    "    for row in itertools.islice(reader, *class_four_data):\n",
    "        print row[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_buf = StringIO.StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1557393927', 'Sun Apr 19 03:50:43 PDT 2009', 'NO_QUERY', 'davidgarlick', '@Natalia_Bella not much to buy now Woolworth closed down ']\n",
      "['0', '1557394133', 'Sun Apr 19 03:50:48 PDT 2009', 'NO_QUERY', 'sarah_etf', 'Kill me please -.- ...Oh crap school tommorow ']\n",
      "['0', '1557394170', 'Sun Apr 19 03:50:48 PDT 2009', 'NO_QUERY', 'hypnotic', \"@chriskeating re the labour general secretary meeting with Labour PM's aide - I posted the very same on facebook. BBC gone downhill \"]\n",
      "['0', '1557394563', 'Sun Apr 19 03:50:57 PDT 2009', 'NO_QUERY', 'Robbertt', 'Whole day of homework ahead of name ']\n",
      "['0', '1557394776', 'Sun Apr 19 03:51:02 PDT 2009', 'NO_QUERY', 'desii____8', \"hamlet...romeo n juliet...radio:ACTIVE live at Wembley...McFly tour DVD's too money to me \"]\n",
      "['0', '1557394848', 'Sun Apr 19 03:51:03 PDT 2009', 'NO_QUERY', 'loubeejones', '@charleypearson haha, lucky you. i just got told one!  loubee is not happy!']\n",
      "['0', '1557395142', 'Sun Apr 19 03:51:09 PDT 2009', 'NO_QUERY', 'musicjunkie92', '@SpecialEmily aw he says thank you! Yea its lush here got dress&amp;flipflops on but i broke my sunnies  gettin new ones @ the beach tho ']\n",
      "['0', '1557395351', 'Sun Apr 19 03:51:14 PDT 2009', 'NO_QUERY', 'Javamomma', '@Bruno108 Ohhh, so much? But I have?! Ohh, I feel terrible! ']\n",
      "['0', '1557395422', 'Sun Apr 19 03:51:16 PDT 2009', 'NO_QUERY', 'magnusholmgren', 'Inspiration is laughing at me ']\n",
      "['0', '1557395551', 'Sun Apr 19 03:51:19 PDT 2009', 'NO_QUERY', 'nammymoo', \"can't believe match was cancelled   is officially gutted. . .\"]\n",
      "['0', '1557395624', 'Sun Apr 19 03:51:21 PDT 2009', 'NO_QUERY', 'Sarraahh95', 'Is sitting at home bored  And doesnt want to go back to school tomorrow']\n",
      "['0', '1557395625', 'Sun Apr 19 03:51:20 PDT 2009', 'NO_QUERY', 'KatesMemories', 'Off to work  Yeah, ad set Sunday!']\n",
      "['0', '1557396071', 'Sun Apr 19 03:51:30 PDT 2009', 'NO_QUERY', 'staceylaura', '100th update. im at uni ']\n",
      "['0', '1557396096', 'Sun Apr 19 03:51:31 PDT 2009', 'NO_QUERY', 'Fernexo', 'has been doin graphics work for aaaaaaaaaaaages now  x']\n",
      "['0', '1557396243', 'Sun Apr 19 03:51:35 PDT 2009', 'NO_QUERY', 'heathermartino', \"OH!  I'll email it to you\"]\n",
      "['0', '1557396312', 'Sun Apr 19 03:51:36 PDT 2009', 'NO_QUERY', 'Spacegirlnz', \"GLaDOS: one of the most engaging characters ever to appear in a videogame ?. If only she hadn't made me kill my companion cube \"]\n",
      "['0', '1557396426', 'Sun Apr 19 03:51:39 PDT 2009', 'NO_QUERY', 'etiago', 'Eating chocolate and back to my data warehouse *weeps* ']\n",
      "['0', '1557396560', 'Sun Apr 19 03:51:43 PDT 2009', 'NO_QUERY', 'megannx', \"happy that i missed coronor's bringin out a nice old lady's body who lived near my dad. she died all on her own \"]\n",
      "['0', '1557396741', 'Sun Apr 19 03:51:46 PDT 2009', 'NO_QUERY', 'PaulaaGeorgee', 'so happy i go back to school on tuesday not tomorrow. I MUST GET @mileycyrus tickets ']\n",
      "['0', '1557396927', 'Sun Apr 19 03:51:51 PDT 2009', 'NO_QUERY', 'bahkti', '@gomezzephyr Ugh sweetie, Ugh ']\n",
      "['0', '1557397005', 'Sun Apr 19 03:51:52 PDT 2009', 'NO_QUERY', 'sarahcole87', 'if officially single after 4.5 years ']\n",
      "['0', '1557397130', 'Sun Apr 19 03:51:57 PDT 2009', 'NO_QUERY', 'meriisaa', \"tomorrow school again  ... i can'T wait so see you all ;)... today doing something for school.. oh i hate it..\"]\n",
      "['0', '1557397201', 'Sun Apr 19 03:51:58 PDT 2009', 'NO_QUERY', 'jaulin', '@dittebb pulled a muscle in the shoulder. I sure hope it gets better soon. ']\n",
      "['0', '1557397249', 'Sun Apr 19 03:51:59 PDT 2009', 'NO_QUERY', 'alexbcann', \"@LyndaJWilson STOP IT! Right now. That's an order  xx\"]\n",
      "['0', '1557397385', 'Sun Apr 19 03:52:02 PDT 2009', 'NO_QUERY', 'MJihad', '@boondocksaint1 Berlin got food poisoning this time ']\n",
      "['0', '1557398038', 'Sun Apr 19 03:52:17 PDT 2009', 'NO_QUERY', 'KISSmyBLAKarts', 'ARGHHHH...have misplaced my latest draft of the comedy show Ive been writing!!!!!! Think I need a break ']\n",
      "['0', '1557398257', 'Sun Apr 19 03:52:22 PDT 2009', 'NO_QUERY', 'lizzie_gordon', 'Preparing myself for work tomorrow. People to call, things to organise... 1 day without work does not count as a weekend ']\n",
      "['0', '1557398649', 'Sun Apr 19 03:52:31 PDT 2009', 'NO_QUERY', 'raquelramosx', '@Winniex No time right now  im packing in 5 minutes roberts packing the computer so no computer for 1 month ']\n",
      "['0', '1557398922', 'Sun Apr 19 03:52:39 PDT 2009', 'NO_QUERY', 'AlanG123', '@audiobullys Would love to see you when your in Inverness But not over 18  lol oh well hopefuly see u some other time !']\n",
      "['0', '1557398933', 'Sun Apr 19 03:52:38 PDT 2009', 'NO_QUERY', 'mktw', '@Meech13 damn it! half way through the interview the sound cuts out, just as u talk abt books! ']\n",
      "['0', '1557399332', 'Sun Apr 19 03:52:47 PDT 2009', 'NO_QUERY', 'wendybird3', 'Oh so sad, Leaky Lounge is not letting me in ']\n",
      "['0', '1557399558', 'Sun Apr 19 03:52:53 PDT 2009', 'NO_QUERY', 'Javamomma', \"may just have a sick house..Youngest son just got up &amp; says &quot;I'm Sick!&quot; \"]\n",
      "['0', '1557399645', 'Sun Apr 19 03:52:54 PDT 2009', 'NO_QUERY', 'mellytx', 'DRINKING SHOTS AND WATCHING INTERVENTION IS LIKE LOOKING IN A MIRROR AND DRINKING :0 ']\n",
      "['0', '1557399718', 'Sun Apr 19 03:52:56 PDT 2009', 'NO_QUERY', 'MoMo_Coleman', 'no time to twitter ']\n",
      "['0', '1557400769', 'Sun Apr 19 03:53:22 PDT 2009', 'NO_QUERY', 'vickerini', 'Am on my own having been forced to leave last day pizza get together by ridiculous phobia of birds. So stressed out ']\n",
      "['0', '1557400887', 'Sun Apr 19 03:53:23 PDT 2009', 'NO_QUERY', 'missnatley', 'relieved to no longer be stuck in the tiny tiny elevator at 1am!!! Oh my claustrophobia ']\n",
      "['0', '1557401034', 'Sun Apr 19 03:53:27 PDT 2009', 'NO_QUERY', 'DonMcAllister', '@paulshadwell Well if it makes you feel any better, we only had Quorn sausages   The rest was nice though!']\n",
      "['0', '1557401163', 'Sun Apr 19 03:53:31 PDT 2009', 'NO_QUERY', 'Cyntech', '@jbekkema Rain + balding tyres + being tired + not really paying attention = hitting L plater... His Mum was a real bitch too  ']\n",
      "['0', '1557401191', 'Sun Apr 19 03:53:30 PDT 2009', 'NO_QUERY', 'Triceln', 'just moved to the couch cause my boo said i was snoring ']\n",
      "['0', '1557401376', 'Sun Apr 19 03:53:36 PDT 2009', 'NO_QUERY', 'WTFJAY', '@LaiRenee Did you get hit with Mikey? ']\n",
      "['0', '1557401525', 'Sun Apr 19 03:53:38 PDT 2009', 'NO_QUERY', 'LeeLeeKins', 'I am bored of doing assignments ']\n",
      "['0', '1557401601', 'Sun Apr 19 03:53:40 PDT 2009', 'NO_QUERY', 'secretXeyes', 'feeling pretty ill today  just wonna go bk to sleep']\n",
      "['0', '1557402029', 'Sun Apr 19 03:53:50 PDT 2009', 'NO_QUERY', 'Ellzie120', \"@McALiMeal  You are never going to be a &quot;fucked up little kid&quot;. If anyone dared say that to you i'd fucking punch them D: you're amazing.\"]\n",
      "['0', '1557402031', 'Sun Apr 19 03:53:49 PDT 2009', 'NO_QUERY', 'Luggy7', '@Kazzy1978 I didnt think I drank that much too. Need to stop drinking on an empty stomach ']\n",
      "['0', '1557402346', 'Sun Apr 19 03:53:56 PDT 2009', 'NO_QUERY', 'Mull_', 'Homework  BORING !!']\n",
      "['0', '1557402387', 'Sun Apr 19 03:53:58 PDT 2009', 'NO_QUERY', 'Elisatee', 'Troy killed the cutest bird....very sad ']\n",
      "['0', '1557402449', 'Sun Apr 19 03:54:00 PDT 2009', 'NO_QUERY', 'Akaike', \"I'm really not looking forward to going into work today   Weather's too nice outside!\"]\n",
      "['0', '1557402636', 'Sun Apr 19 03:54:02 PDT 2009', 'NO_QUERY', 'Teppotastic', \"@LoneStarshine oh no! I'm very very sorry to hear that \"]\n",
      "['0', '1557402906', 'Sun Apr 19 03:54:08 PDT 2009', 'NO_QUERY', 'DevilsRefugee', 'NASA may need extra $30b to stay on schedule to moon http://ff.im/-2dood (via @gaz4695) What prices Mars ?? ']\n",
      "['0', '1557405071', 'Sun Apr 19 03:54:59 PDT 2009', 'NO_QUERY', 'taffysaint', 'just waiting on the netbbook to charge up then a lovely day at the office. ']\n",
      "['4', '1563988954', 'Mon Apr 20 01:12:32 PDT 2009', 'NO_QUERY', 'nibarryc', 'finally, hand in date for an assignment, everyone is online and in a panic lol  me toooooooo']\n",
      "['4', '1563988999', 'Mon Apr 20 01:12:33 PDT 2009', 'NO_QUERY', 'annabethblue', \"@thechrisgriffin I don't know...this is the first I've heard of Avril.   Are you secretly a sk8tr boi? Or...do you like sk8tr bois? ;P\"]\n",
      "['4', '1563989094', 'Mon Apr 20 01:12:35 PDT 2009', 'NO_QUERY', 'ArticDesign', 'Happy Monday everyone  We\\xef\\xbf\\xbdre back at work!']\n",
      "['4', '1563989162', 'Mon Apr 20 01:12:37 PDT 2009', 'NO_QUERY', 'mscofino', \"@chamada Will it be your first time to Italy? You're going to love it! My absolute favorite place \"]\n",
      "['4', '1563989263', 'Mon Apr 20 01:12:38 PDT 2009', 'NO_QUERY', 'fotofrancis', 'Trying to get all things together for my trip to Riga and all things needed for immigration purposes... ']\n",
      "['4', '1563989295', 'Mon Apr 20 01:12:39 PDT 2009', 'NO_QUERY', 'jojobarney', 'loves Eastlink... 10 mins from Blackburn Rd  to Eastlink on High Street Rd... 3 mins to Ringwood. ']\n",
      "['4', '1563989302', 'Mon Apr 20 01:12:39 PDT 2009', 'NO_QUERY', 'djpoppinfresh', '&quot;Saw&quot; The game is scheduled to release in 2009 ']\n",
      "['4', '1563989313', 'Mon Apr 20 01:12:40 PDT 2009', 'NO_QUERY', 'Floris', '@mezzle Undisturbed 8 hour sleep. And not waking up with hangover == good premise for the day ']\n",
      "['4', '1563989494', 'Mon Apr 20 01:12:43 PDT 2009', 'NO_QUERY', 'Jordan_M', \"@DocAdams well I loved the first so I'll jump straight on this one! \"]\n",
      "['4', '1563989495', 'Mon Apr 20 01:12:43 PDT 2009', 'NO_QUERY', 'rajeshlalwani', '@codelust that is also true ']\n",
      "['4', '1563989510', 'Mon Apr 20 01:12:43 PDT 2009', 'NO_QUERY', 'effingcards', '@NatskiB glad you like! now buy some ']\n",
      "['4', '1563989523', 'Mon Apr 20 01:12:43 PDT 2009', 'NO_QUERY', 'Saresa', '@mattycus /hug.  Whatever you think is best for you ']\n",
      "['4', '1563989570', 'Mon Apr 20 01:12:44 PDT 2009', 'NO_QUERY', 'XSandOSx6', '@TheBigfella no tickets here. we got ya ']\n",
      "['4', '1563989578', 'Mon Apr 20 01:12:45 PDT 2009', 'NO_QUERY', 'xoxoshahirah', 'Bye guys. Gonna watch my show and do my school work at the same time! Multitasking! ']\n",
      "['4', '1563989592', 'Mon Apr 20 01:12:44 PDT 2009', 'NO_QUERY', 'MisterRo', \"@bryantma Not a bad start, but don't forget that my bruiser's a little older than yours; he's already had his education from me \"]\n",
      "['4', '1563989593', 'Mon Apr 20 01:12:45 PDT 2009', 'NO_QUERY', 'Obelina220', \"@Sarr3o7 Restaurant city addict! who can get a pass in ECons by playing it. haha I know exactly how you'll look at me.stop hahaha ILY! \"]\n",
      "['4', '1563989670', 'Mon Apr 20 01:12:46 PDT 2009', 'NO_QUERY', 'ivyclark', '@minapaige hi mina, how are you? Have not heard from you for a long time. ']\n",
      "['4', '1563989677', 'Mon Apr 20 01:12:47 PDT 2009', 'NO_QUERY', 'Princessgigi90', '@MariahCarey I want to let u kno that u are my fav artist of all time. i truly admire u and wish u the best...hope to hear from u!! ']\n",
      "['4', '1563989685', 'Mon Apr 20 01:12:47 PDT 2009', 'NO_QUERY', 'bellarr', 'eatingg yummy lindtt chocolatee!  woohoo!']\n",
      "['4', '1563989703', 'Mon Apr 20 01:12:47 PDT 2009', 'NO_QUERY', 'manatmouse', \"hi @RyanSeacrest! I would suggest music of my electro project *** http://bit.ly/12KoF0 *** fresh, kickin', different! free dl &amp; have fun \"]\n",
      "['4', '1563989738', 'Mon Apr 20 01:12:48 PDT 2009', 'NO_QUERY', 'bendemora', '@KikkerToo Ford Focus CC-3. Bloody excellent car - just needs a space saver wheel in the boot ']\n",
      "['4', '1563989765', 'Mon Apr 20 01:12:48 PDT 2009', 'NO_QUERY', 'jeanelakin', \"@physicsphaery No I didn't get it yet, and you confirmed my suspicions. And gave me more!  Thank you!\"]\n",
      "['4', '1563989766', 'Mon Apr 20 01:12:48 PDT 2009', 'NO_QUERY', 'MonkyMagic', \"@MooMoo_82 hehehe v.true, if only it sent sms's to our mobiles (like in the US/UK) then would be easier to follow =/ Im good, studyin... \"]\n",
      "['4', '1563989793', 'Mon Apr 20 01:12:49 PDT 2009', 'NO_QUERY', 'i386', 'Blogged our new offer ']\n",
      "['4', '1563989890', 'Mon Apr 20 01:12:51 PDT 2009', 'NO_QUERY', 'christinebabyyy', 'i neeeeeeeed to peeeeeeee ']\n",
      "['4', '1563990074', 'Mon Apr 20 01:12:55 PDT 2009', 'NO_QUERY', 'kisvirag', '@theitalianjob: az se rossz ']\n",
      "['4', '1563990090', 'Mon Apr 20 01:12:56 PDT 2009', 'NO_QUERY', 'akeysandjg', 'is bored as CRAP.  Entertain!!!!! ']\n",
      "['4', '1563990135', 'Mon Apr 20 01:12:56 PDT 2009', 'NO_QUERY', 'ShropshirePixie', '@Col_RFTL thank you honey! I can play it again now    do I refer to you as Gandalf from now on ;-)']\n",
      "['4', '1563990219', 'Mon Apr 20 01:12:58 PDT 2009', 'NO_QUERY', 'FKrulestheworld', 'Smile, and the world will smile back to you ']\n",
      "['4', '1563990241', 'Mon Apr 20 01:12:58 PDT 2009', 'NO_QUERY', 'rafifyalda', \"@atebits it's 6pm here, no need for sleep yet \"]\n",
      "['4', '1563990267', 'Mon Apr 20 01:12:58 PDT 2009', 'NO_QUERY', 'iamolly', 'Just back from walking the dog, catching up on emails and stuff after a weekend gardening ']\n",
      "['4', '1563990294', 'Mon Apr 20 01:12:59 PDT 2009', 'NO_QUERY', 'JeanetteLim', 'filter that baby bump that track ']\n",
      "['4', '1563990312', 'Mon Apr 20 01:12:59 PDT 2009', 'NO_QUERY', 'gypsyraven', 'Bored of Victoria, booking flights &amp; hotel to party at Seattle again  ']\n",
      "['4', '1563990415', 'Mon Apr 20 01:13:02 PDT 2009', 'NO_QUERY', 'SourRing', '@yen_menthol Tell me about it. Have a goodnight ']\n",
      "['4', '1563990451', 'Mon Apr 20 01:13:02 PDT 2009', 'NO_QUERY', 'Justin_A', \"@NovaWildstar that's good! I'm not properly awake  yet and I got up at 6am. \"]\n",
      "['4', '1563990465', 'Mon Apr 20 01:13:03 PDT 2009', 'NO_QUERY', 'vivianleexo', 'Themeeee park --- Roller coasters - loveee ']\n",
      "['4', '1563990500', 'Mon Apr 20 01:13:04 PDT 2009', 'NO_QUERY', 'mharis', '@Senilius_110 Oh, enjoy your new job. Good luck ']\n",
      "['4', '1563990533', 'Mon Apr 20 01:13:04 PDT 2009', 'NO_QUERY', 'Alice_Cullen_x', \"@Rachealblack110 I'm great sweetie  How are you? x\"]\n",
      "['4', '1563990586', 'Mon Apr 20 01:13:05 PDT 2009', 'NO_QUERY', 'rikerpkr', '@evybabee you look like you can play a mean guitar too and cards  ~scott']\n",
      "['4', '1563990620', 'Mon Apr 20 01:13:05 PDT 2009', 'NO_QUERY', 'aaronbordin', ' at robclark182']\n",
      "['4', '1563990627', 'Mon Apr 20 01:13:05 PDT 2009', 'NO_QUERY', 'joenoia', 'Rt @Birgit_ @joenoia douche in my native language means &quot;shower&quot;  http://tinyurl.com/axwru']\n",
      "['4', '1563990670', 'Mon Apr 20 01:13:06 PDT 2009', 'NO_QUERY', 'EloraDanan', '@tatut It was my mum who got the tickets ']\n",
      "['4', '1563990771', 'Mon Apr 20 01:13:09 PDT 2009', 'NO_QUERY', 'joolzgirl', '@oliyoung @radiostarelle #herebeforeoprah haha good one ']\n",
      "['4', '1563990774', 'Mon Apr 20 01:13:08 PDT 2009', 'NO_QUERY', 'Bloke28', \"@levimorales Yeah now the sun really does shine outta my . Profile ..  Got sick of the lawn look, thought i'd make it look a little better\"]\n",
      "['4', '1563990800', 'Mon Apr 20 01:13:09 PDT 2009', 'NO_QUERY', 'KurtSwann', '@WomenCan gotta love twitter! i learn something new everyday ']\n",
      "['4', '1563990836', 'Mon Apr 20 01:13:08 PDT 2009', 'NO_QUERY', 'KiwiKisses', \"What Cock you dnt have one lil boy and esides i bet jessica will... NOT cuz you're like hel rank and all \"]\n",
      "['4', '1563990856', 'Mon Apr 20 01:13:10 PDT 2009', 'NO_QUERY', 'CheekyLou', 'Had such a relaxing weekend &amp; am ready for the busy week of designing that I have ahead of me ']\n",
      "['4', '1563990864', 'Mon Apr 20 01:13:09 PDT 2009', 'NO_QUERY', 'tasshh', '@Umbrella_Skies i hate you rn! only not really...ILY &lt;3 but i am very jealous! lol, have fun ']\n",
      "['4', '1563990927', 'Mon Apr 20 01:13:11 PDT 2009', 'NO_QUERY', 'denisefarabee', '@jacivelasquez quote on myspace from you silly  .. &quot; Jaci Velasquez\\xef\\xbf\\xbdFree download available&quot; one prob no link????? \\xef\\xbf\\xbdJoy williams links\\xef\\xbf\\xbd!!!']\n",
      "['4', '1563991014', 'Mon Apr 20 01:13:13 PDT 2009', 'NO_QUERY', 'LadyLyric88', 'OMGoshness!! britney spears is following me!! lol thankz britz ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in string_buf.getvalue().split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. All html tags and attributes (i.e., /<[^>]+>/) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(tweet):\n",
    "    return re.sub(r'<[^>]+>', '', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Want This text!'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_html_tags('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Html character codes (i.e., &...;) are replaced with an ASCII equivalent.\n",
    "- Remove the ascii encoding to support extended in unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_html_codes(tweet):\n",
    "    parser = HTMLParser.HTMLParser()\n",
    "    tweet = filter(lambda x: x in string.printable, tweet)\n",
    "    return parser.unescape(tweet).encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You win 100  \" $\n"
     ]
    }
   ],
   "source": [
    "print replace_html_codes('&quot;You win &pound;100 &iexcl; &quot; &#36;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were back at work\n"
     ]
    }
   ],
   "source": [
    "print replace_html_codes(\"We�re back at work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. All URLs (i.e., tokens beginning with http or www) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_urls(tweet):\n",
    "    # Note that this will modify the whitespace when words are separated by\n",
    "    # more than one space, but that shouldn't matter as we are tokenizing\n",
    "    # the tweets anyways\n",
    "    \n",
    "    return ' '.join(filter(lambda x : not x.lower().startswith(('www', 'http')), tweet.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brad is the best dancer but not the worst singer'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(\"brad is the best www.youtube.com dancer but not the worst http://www.google.ca singer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The first character in Twitter user names (@) and hash tags (#) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet):\n",
    "    return ' '.join([ x[1:] if  x.startswith(('@', '#')) else x for x in tweet.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brad donkey kick face @other #test'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hashtags('brad #donkey @kick face #@other @#test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Each sentence within a tweet is on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_abbrev_set(file_path='abbrev.english'):\n",
    "    abbrev_set = set()\n",
    "    \n",
    "    with open(file_path, 'rb') as abbrevs:\n",
    "        for line in abbrevs:\n",
    "            abbrev_set.add(line.strip())\n",
    "            #print line.strip()\n",
    "    \n",
    "    return abbrev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abbrev_set = create_abbrev_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print 'Mr.' in abbrev_set\n",
    "print 'Pa.' in abbrev_set\n",
    "print 'Pr.' in abbrev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_by_sentence(tweet):\n",
    "    '''\n",
    "        # 1. Anything ending in .?! declared a sentence\n",
    "        # 2. Sentence boundary moved after quotation mark, if any ex. He said, \"I am coming.\"\n",
    "        # 3. Period boundary is disqualified if it preceded by an element in abbrev_set\n",
    "        #    <We could look for capitals after an EOS, but nobody uses capitals on twitter>\n",
    "        #    <Both sides of :;- could also be thought of as sentence>\n",
    "    '''\n",
    "    \n",
    "    tweet = re.sub(r' +', ' ', tweet).strip()\n",
    "    abbrev_set = create_abbrev_set()\n",
    "    split_by_space = tweet.split(' ')\n",
    "\n",
    "    quote_eos = lambda x: len(x) > 1 and (x[-2:] in {'.\"', '?\"', '!\"'} or x[-2:] in {\".'\", \"?'\", \"!'\"})\n",
    "    eos = lambda x: (x[-1] in {'.', '?', '!'} and x not in abbrev_set) or (quote_eos(x) and x[:-1] not in abbrev_set)\n",
    "    eos_indices = [i + 1 for i, x in enumerate(split_by_space) if eos(x)]\n",
    "    \n",
    "    if 0 == len(eos_indices):\n",
    "        return [tweet]\n",
    "    \n",
    "    sents = [' '.join(x) for x in [split_by_space[i:j] for i, j in zip([0] + eos_indices[:-1], eos_indices)]]\n",
    "\n",
    "    if eos_indices[-1] < len(split_by_space):\n",
    "        sents = sents + [' '.join(split_by_space[eos_indices[-1]:])]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_tweet = '4km technique swim set done Mr.\" Meeting with a creative director at 10am.\" Photographs to the printers. A million phone calls to make.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4km technique swim set done Mr.\" Meeting with a creative director at 10am.\"',\n",
       " 'Photographs to the printers.',\n",
       " 'A million phone calls to make.']"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_sentence(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meech13 damn it!!!!, \\xc2\\xa1 half. Mr. Mr.' way text! through the interview @the sound cut's out, just' as u talk abt books!\""
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Meech13 damn it!!!!, ¡ half. Mr. Mr.' way text! through the interview @the sound cut's out, just' as u talk abt books!\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meech13 damn it!!!!, \\xc2\\xa1 half.',\n",
       " \"Mr. Mr.' way text!\",\n",
       " \"through the interview @the sound cut's out, just' as u talk abt books!\"]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = split_by_sentence(test)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brad is the best']"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = split_by_sentence(\"brad is the best\")\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpecialEmily aw he says thank you! Yea its lush here got dress&flipflops on but i broke my sunnies gettin new ones  the beach tho'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = \"SpecialEmily aw he says thank you! Yea its lush here got dress&flipflops on but i broke my sunnies gettin new ones  the beach tho\"\n",
    "test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpecialEmily aw he says thank you!',\n",
       " 'Yea its lush here got dress&flipflops on but i broke my sunnies gettin new ones the beach tho']"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_sentence(test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6/7. Each token, including punctuation and clitics, is separated by spaces.\n",
    "- Clitics: contracted forms of words, such as n’t\n",
    "- 's on possessive (ie. Brad's) different from 's on clitics (ie. What's), but both separated\n",
    "- Must also separate possessive on plurals (ie. dogs ')\n",
    "- Ellipsis (i.e., ‘...’), and other kinds of multiple punctuation (e.g., ‘!!!’) are not split.\n",
    "- Don't split e.g. into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_tokens(sentence):\n",
    "    # 1. Split on all punctuation symbols, where a given symbol is repeated once or more\n",
    "    sentence_1 = re.sub(r\"(([\"+ string.punctuation + \"])\\\\2*)\", r\" \\1 \", sentence).strip()\n",
    "    sentence_1 = ' '.join(sentence_1.split('  '))\n",
    "    \n",
    "    # 2. Join clitics and contractions where ' occurs mid-word\n",
    "    sentence_2 = re.sub(r\"(') ([A-Za-z] )\", r\"\\1\\2\", sentence_1)\n",
    "    \n",
    "    # 3. Join e.g.\n",
    "    sentence_3 = re.sub(r\" e . g . \", r\" e.g. \", sentence_2)\n",
    "    \n",
    "    return sentence_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"... Brad's dog hasn't said that the cereal is the dogs' or anyone elses..., but, we know  e.g. better????\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"... Brad 's dog hasn 't said that the cereal is the dogs ' or anyone elses ... , but , we know e.g. better ????\""
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tokens(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Each token is tagged with its part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_sentence(sentence, pos_tagger):\n",
    "    '''\n",
    "    Assume sentence is already separated into tokens\n",
    "    '''\n",
    "    split = sentence.split(' ')\n",
    "    return ' '.join([x[0] + \"/\" + x[1] for x in zip(split, pos_tagger.tag(split))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp.NLPlib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = \"Meet me today at the FEC in DC at 4 .\"\n",
    "sentence2 = \"Wear a carnation so I know it 's you .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
      "Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\n"
     ]
    }
   ],
   "source": [
    "# Expected:\n",
    "# Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
    "# Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP ’s/POS you/PRP ./.\n",
    "print tag_sentence(sentence1, tagger)\n",
    "print tag_sentence(sentence2, tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Before each tweet is demarcation A=# in <> which occurs on its own line, where # is the numeric class of the tweet (0, 2, or 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_class(sentences, class_):\n",
    "    prepend = \"<A={}>\".format(class_)\n",
    "    return [prepend] + sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<A=4>',\n",
       " 'Meet me today at the FEC in DC at 4 .',\n",
       " \"Wear a carnation so I know it 's you .\"]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_class([sentence1, sentence2], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweet = \"@Meech13 damn it!!!!, half. Mr. Mr.' way <b>text!</b> through http://www.google.ca #the      interview #@the sound cut's out, just' as&#36; u talk abt books!   \"\n",
    "test_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tweet, t_class, tagger):\n",
    "    tweet = re.sub(r' +', ' ', tweet).strip()\n",
    "    tweet = strip_html_tags(tweet)\n",
    "    tweet = replace_html_codes(tweet)\n",
    "    tweet = remove_urls(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "\n",
    "    sentences = split_by_sentence(tweet)\n",
    "    sentences = [split_tokens(sentence) for sentence in sentences]\n",
    "    sentences = [tag_sentence(sentence, tagger) for sentence in sentences]\n",
    "    sentences = add_class(sentences, t_class)\n",
    "    \n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp.NLPlib()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<A=0>',\n",
       " 'Meech13/NN damn/JJ it/PRP !!!!/NN ,/, half/NN ./.',\n",
       " \"Mr/NNP ./. Mr/NNP ./. '/POS way/NN text/NN !/.\",\n",
       " \"through/IN the/DT interview/NN @/IN the/DT sound/NN cut/VB 's/POS out/IN ,/, just/RB '/POS as/IN $/$ u/PRP talk/VB abt/NN books/NNS !/.\"]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test_tweet, test_class, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twtt(output_file, input_file='training.1600000.processed.noemoticon.csv', GID=4):\n",
    "    class_zero_data = [GID * 5500, GID * 5500 + 50] # (GID + 1) * 5500 - 1]\n",
    "    class_four_data = [GID * 5500 + 800000, GID * 5500 + 800000 + 50] # (GID + 1) * 5500 - 1 + 800000]\n",
    "    \n",
    "    with open(input_file, 'r') as train_file:\n",
    "        reader = csv.reader(train_file)\n",
    "\n",
    "        tagger = nlp.NLPlib()\n",
    "        \n",
    "        for row in itertools.islice(reader, *class_zero_data):\n",
    "            tweet = row[5]\n",
    "            t_class = 0\n",
    "            sentences = preprocess(tweet, t_class, tagger)\n",
    "            for sentence in sentences:\n",
    "                output_file.write(sentence + '\\n')\n",
    "                \n",
    "        for row in itertools.islice(reader, *class_four_data):\n",
    "            tweet = row[5]\n",
    "            t_class = 4\n",
    "            sentences = preprocess(tweet, t_class, tagger)\n",
    "            for sentence in sentences:\n",
    "                output_file.write(sentence + '\\n')\n",
    "        \n",
    "        return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StringIO.StringIO instance at 0x10609a1b8>"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_buf = StringIO.StringIO()\n",
    "twtt(output_buf, 'training.1600000.processed.noemoticon.csv', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<A=0>\n",
      "Natalia/NNP _/NN Bella/NNP not/RB much/JJ to/TO buy/VB now/RB Woolworth/NNP closed/VBD down/RB\n",
      "<A=0>\n",
      "Kill/VB me/PRP please/VB -/: ./. -/: /NN .../: Oh/UH crap/NN school/NN tommorow/NN\n",
      "<A=0>\n",
      "chriskeating/VBG re/NN the/DT labour/NN general/JJ secretary/NN meeting/VBG with/IN Labour/NNP PM/NNP 's/POS aide/NN -/: I/PRP posted/VBD the/DT very/RB same/JJ on/IN facebook/NN ./.\n",
      "BBC/NNP gone/VBN downhill/RB\n",
      "<A=0>\n",
      "Whole/JJ day/NN of/IN homework/NN ahead/RB of/IN name/NN\n",
      "<A=0>\n",
      "hamlet/NN .../: romeo/NN n/NN juliet/NN .../: radio/NN :/: ACTIVE/JJ live/VB at/IN Wembley/NN .../: McFly/RB tour/NN DVD/NN 's/POS too/RB money/NN to/TO me/PRP\n",
      "<A=0>\n",
      "charleypearson/NN haha/NN ,/, lucky/JJ you/PRP ./.\n",
      "i/NN just/RB got/VBD told/VBD one/CD !/.\n",
      "loubee/NN is/VBZ not/RB happy/JJ !/.\n",
      "<A=0>\n",
      "SpecialEmily/RB aw/UH he/PRP says/VBZ thank/VB you/PRP !/.\n",
      "Yea/UH its/PRP$ lush/JJ here/RB got/VBD dress/NNS &/CC flipflops/NNS on/IN but/CC i/NN broke/VBD my/PRP$ sunnies/NNS gettin/VBG new/JJ ones/NNS the/DT beach/NN tho/NN\n",
      "<A=0>\n",
      "Bruno108/NN Ohhh/NN ,/, so/RB much/JJ ?/.\n",
      "But/CC I/PRP have/VBP ?/. !/.\n",
      "Ohh/NN ,/, I/PRP feel/VB terrible/JJ !/.\n",
      "<A=0>\n",
      "Inspiration/NN is/VBZ laughing/VBG at/IN me/PRP\n",
      "<A=0>\n",
      "can/MD 't/NN believe/VBP match/VB was/VBD cancelled/VBN is/VBZ officially/RB gutted/VBN ./.\n",
      "./.\n",
      "./.\n",
      "<A=0>\n",
      "Is/VBZ sitting/VBG at/IN home/NN bored/VBN And/CC doesnt/NN want/VBP to/TO go/VB back/RB to/TO school/NN tomorrow/NN\n",
      "<A=0>\n",
      "Off/IN to/TO work/NN Yeah/UH ,/, ad/NN set/VBN Sunday/NNP !/.\n",
      "<A=0>\n",
      "100th/NN update/VB ./.\n",
      "im/NN at/IN uni/NN\n",
      "<A=0>\n",
      "has/VBZ been/VBN doin/VBG graphics/NNS work/NN for/IN aaaaaaaaaaaages/NNS now/RB x/NN\n",
      "<A=0>\n",
      "OH/NN !/.\n",
      "I/PRP '/POS ll/NN email/NN it/PRP to/TO you/PRP\n",
      "<A=0>\n",
      "GLaDOS/NN :/: one/CD of/IN the/DT most/RBS engaging/VBG characters/NNS ever/RB to/TO appear/VB in/IN a/DT videogame/NN ?/. ./.\n",
      "If/IN only/RB she/PRP hadn/NN 't/NN made/VBN me/PRP kill/VB my/PRP$ companion/NN cube/NN\n",
      "<A=0>\n",
      "Eating/VBG chocolate/NN and/CC back/RB to/TO my/PRP$ data/NNS warehouse/NN */SYM weeps/NNS */SYM\n",
      "<A=0>\n",
      "happy/JJ that/IN i/NN missed/VBD coronor/NN 's/POS bringin/NN out/IN a/DT nice/JJ old/JJ lady/NN 's/POS body/NN who/WP lived/VBD near/IN my/PRP$ dad/NN ./.\n",
      "she/PRP died/VBD all/DT on/IN her/PRP$ own/JJ\n",
      "<A=0>\n",
      "so/RB happy/JJ i/NN go/VB back/RB to/TO school/NN on/IN tuesday/NN not/RB tomorrow/NN ./.\n",
      "I/PRP MUST/MD GET/VB mileycyrus/NNS tickets/NNS\n",
      "<A=0>\n",
      "gomezzephyr/NN Ugh/UH sweetie/NN ,/, Ugh/UH\n",
      "<A=0>\n",
      "if/IN officially/RB single/JJ after/IN 4/NN ./. 5/NN years/NNS\n",
      "<A=0>\n",
      "tomorrow/NN school/NN again/RB .../:\n",
      "i/NN can/MD 'T/NN wait/VB so/RB see/VB you/PRP all/DT ;/: )/) .../:\n",
      "today/NN doing/VBG something/VBG for/IN school/NN ../.\n",
      "oh/UH i/NN hate/VBP it/PRP ../.\n",
      "<A=0>\n",
      "dittebb/NN pulled/VBD a/DT muscle/NN in/IN the/DT shoulder/NN ./.\n",
      "I/PRP sure/JJ hope/NN it/PRP gets/VBZ better/JJR soon/RB ./.\n",
      "<A=0>\n",
      "LyndaJWilson/NN STOP/VB IT/PRP !/.\n",
      "Right/RB now/RB ./.\n",
      "That/DT 's/POS an/DT order/NN xx/NN\n",
      "<A=0>\n",
      "boondocksaint1/NN Berlin/NNP got/VBD food/NN poisoning/VBG this/DT time/NN\n",
      "<A=0>\n",
      "ARGHHHH/NN .../: have/VBP misplaced/VBN my/PRP$ latest/JJS draft/NN of/IN the/DT comedy/NN show/NN Ive/NN been/VBN writing/VBG !!!!!!/NN\n",
      "Think/VBP I/PRP need/VBN a/DT break/NN\n",
      "<A=0>\n",
      "Preparing/VBG myself/PRP for/IN work/NN tomorrow/NN ./.\n",
      "People/NNS to/TO call/VB ,/, things/NNS to/TO organise/NN .../:\n",
      "1/NN day/NN without/IN work/NN does/VBZ not/RB count/NN as/IN a/DT weekend/NN\n",
      "<A=0>\n",
      "Winniex/NN No/DT time/NN right/NN now/RB im/NN packing/VBG in/IN 5/NN minutes/NNS roberts/NNS packing/VBG the/DT computer/NN so/RB no/DT computer/NN for/IN 1/NN month/NN\n",
      "<A=0>\n",
      "audiobullys/NNS Would/MD love/VB to/TO see/VB you/PRP when/WRB your/PRP$ in/IN Inverness/NNP But/CC not/RB over/IN 18/NN lol/NN oh/UH well/RB hopefuly/RB see/VB u/PRP some/DT other/JJ time/NN !/.\n",
      "<A=0>\n",
      "Meech13/NN damn/JJ it/PRP !/.\n",
      "half/NN way/NN through/IN the/DT interview/NN the/DT sound/NN cuts/NNS out/IN ,/, just/RB as/IN u/PRP talk/VB abt/NN books/NNS !/.\n",
      "<A=0>\n",
      "Oh/UH so/RB sad/JJ ,/, Leaky/JJ Lounge/NNP is/VBZ not/RB letting/VBG me/PRP in/IN\n",
      "<A=0>\n",
      "may/MD just/RB have/VBP a/DT sick/JJ house/NN ../. Youngest/JJS son/NN just/RB got/VBD up/IN &/CC says/VBZ \"/\" I/PRP 'm/VBP Sick/NNP !/. \"/\"\n",
      "<A=0>\n",
      "DRINKING/NN SHOTS/NNS AND/CC WATCHING/VBG INTERVENTION/NN IS/VBZ LIKE/IN LOOKING/VBG IN/IN A/DT MIRROR/NN AND/CC DRINKING/NN :/: 0/NN\n",
      "<A=0>\n",
      "no/DT time/NN to/TO twitter/NN\n",
      "<A=0>\n",
      "Am/NNP on/IN my/PRP$ own/JJ having/VBG been/VBN forced/VBN to/TO leave/VB last/JJ day/NN pizza/NN get/VB together/RB by/IN ridiculous/JJ phobia/NN of/IN birds/NNS ./.\n",
      "So/RB stressed/VBD out/IN\n",
      "<A=0>\n",
      "relieved/VBN to/TO no/DT longer/RB be/VB stuck/VBN in/IN the/DT tiny/JJ tiny/JJ elevator/NN at/IN 1am/NN !!!/NN\n",
      "Oh/UH my/PRP$ claustrophobia/NN\n",
      "<A=0>\n",
      "paulshadwell/NN Well/UH if/IN it/PRP makes/VBZ you/PRP feel/VB any/DT better/JJR ,/, we/PRP only/RB had/VBD Quorn/NN sausages/NNS The/DT rest/NN was/VBD nice/JJ though/IN !/.\n",
      "<A=0>\n",
      "jbekkema/NN Rain/NNP +/SYM balding/JJ tyres/NNS +/SYM being/VBG tired/VBN +/SYM not/RB really/RB paying/VBG attention/NN =/SYM hitting/VBG L/NNP plater/NN .../:\n",
      "His/PRP$ Mum/JJ was/VBD a/DT real/JJ bitch/NN too/RB\n",
      "<A=0>\n",
      "just/RB moved/VBD to/TO the/DT couch/NN cause/NN my/PRP$ boo/VB said/VBD i/NN was/VBD snoring/VBG\n",
      "<A=0>\n",
      "LaiRenee/NN Did/VBD you/PRP get/VB hit/VBD with/IN Mikey/NN ?/.\n",
      "<A=0>\n",
      "I/PRP am/VBP bored/VBN of/IN doing/VBG assignments/NNS\n",
      "<A=0>\n",
      "feeling/VBG pretty/RB ill/JJ today/NN just/RB wonna/NN go/VB bk/NN to/TO sleep/VB\n",
      "<A=0>\n",
      "McALiMeal/JJ You/PRP are/VBP never/RB going/VBG to/TO be/VB a/DT \"/\" fucked/VBN up/IN little/JJ kid/NN \"/\" ./.\n",
      "If/IN anyone/NN dared/VBD say/VBP that/IN to/TO you/PRP i/NN 'd/MD fucking/VBG punch/NN them/PRP D/NN :/: you/PRP '/POS re/NN amazing/JJ ./.\n",
      "<A=0>\n",
      "Kazzy1978/NN I/PRP didnt/NN think/VBP I/PRP drank/VBD that/IN much/JJ too/RB ./.\n",
      "Need/VB to/TO stop/VB drinking/VBG on/IN an/DT empty/JJ stomach/NN\n",
      "<A=0>\n",
      "Homework/NN BORING/JJ !!/NN\n",
      "<A=0>\n",
      "Troy/NNP killed/VBN the/DT cutest/JJS bird/NN ..../CD very/RB sad/JJ\n",
      "<A=0>\n",
      "I/PRP 'm/VBP really/RB not/RB looking/VBG forward/RB to/TO going/VBG into/IN work/NN today/NN Weather/NNP 's/POS too/RB nice/JJ outside/IN !/.\n",
      "<A=0>\n",
      "LoneStarshine/NN oh/UH no/DT !/.\n",
      "I/PRP 'm/VBP very/RB very/RB sorry/JJ to/TO hear/VB that/IN\n",
      "<A=0>\n",
      "NASA/NNP may/MD need/VBN extra/JJ $/$ 30b/NN to/TO stay/VB on/IN schedule/NN to/TO moon/NN (/( via/IN gaz4695/NN )/) What/WP prices/NNS Mars/NNP ??/NN\n",
      "<A=0>\n",
      "just/RB waiting/VBG on/IN the/DT netbbook/NN to/TO charge/NN up/IN then/RB a/DT lovely/RB day/NN at/IN the/DT office/NN ./.\n",
      "<A=4>\n",
      "finally/RB ,/, hand/NN in/IN date/NN for/IN an/DT assignment/NN ,/, everyone/NN is/VBZ online/JJ and/CC in/IN a/DT panic/NN lol/NN me/PRP toooooooo/NN\n",
      "<A=4>\n",
      "thechrisgriffin/NN I/PRP don/VB 't/NN know/VB .../: this/DT is/VBZ the/DT first/JJ I/PRP '/POS ve/NN heard/VBN of/IN Avril/NN ./.\n",
      "Are/VBP you/PRP secretly/RB a/DT sk8tr/NN boi/NN ?/.\n",
      "Or/CC .../: do/VBP you/PRP like/IN sk8tr/NN bois/FW ?/.\n",
      ";/: P/NN\n",
      "<A=4>\n",
      "Happy/NNP Monday/NNP everyone/NN Were/VBD back/RB at/IN work/NN !/.\n",
      "<A=4>\n",
      "chamada/NN Will/MD it/PRP be/VB your/PRP$ first/JJ time/NN to/TO Italy/RB ?/.\n",
      "You/PRP '/POS re/NN going/VBG to/TO love/NN it/PRP !/.\n",
      "My/PRP$ absolute/JJ favorite/JJ place/NN\n",
      "<A=4>\n",
      "Trying/VBG to/TO get/VB all/DT things/NNS together/RB for/IN my/PRP$ trip/NN to/TO Riga/NNP and/CC all/DT things/NNS needed/VBN for/IN immigration/NN purposes/NNS .../:\n",
      "<A=4>\n",
      "loves/VBZ Eastlink/NN .../:\n",
      "10/NN mins/NNS from/IN Blackburn/NNP Rd/NN to/TO Eastlink/NN on/IN High/NNP Street/NNP Rd/NN .../:\n",
      "3/NN mins/NNS to/TO Ringwood/NNP ./.\n",
      "<A=4>\n",
      "\"/\" Saw/VBD \"/\" The/DT game/NN is/VBZ scheduled/VBN to/TO release/NN in/IN 2009/NN\n",
      "<A=4>\n",
      "mezzle/NN Undisturbed/JJ 8/NN hour/NN sleep/VB ./.\n",
      "And/CC not/RB waking/VBG up/IN with/IN hangover/NN ==/NN good/JJ premise/NN for/IN the/DT day/NN\n",
      "<A=4>\n",
      "DocAdams/NNS well/RB I/PRP loved/VBD the/DT first/JJ so/RB I/PRP '/POS ll/NN jump/NN straight/JJ on/IN this/DT one/CD !/.\n",
      "<A=4>\n",
      "codelust/NN that/IN is/VBZ also/RB true/JJ\n",
      "<A=4>\n",
      "NatskiB/NN glad/JJ you/PRP like/IN !/.\n",
      "now/RB buy/VB some/DT\n",
      "<A=4>\n",
      "mattycus/NNS //NN hug/NN ./.\n",
      "Whatever/WDT you/PRP think/VBP is/VBZ best/JJS for/IN you/PRP\n",
      "<A=4>\n",
      "TheBigfella/NN no/DT tickets/NNS here/RB ./.\n",
      "we/PRP got/VBD ya/NN\n",
      "<A=4>\n",
      "Bye/UH guys/NNS ./.\n",
      "Gonna/VBG watch/VB my/PRP$ show/NN and/CC do/VBP my/PRP$ school/NN work/NN at/IN the/DT same/JJ time/NN !/.\n",
      "Multitasking/VBG !/.\n",
      "<A=4>\n",
      "bryantma/NN Not/RB a/DT bad/JJ start/VB ,/, but/CC don/VB 't/NN forget/VB that/IN my/PRP$ bruiser/NN 's/POS a/DT little/JJ older/JJR than/IN yours/PRP ;/: he/PRP 's/POS already/RB had/VBD his/PRP$ education/NN from/IN me/PRP\n",
      "<A=4>\n",
      "Sarr3o7/NN Restaurant/NNP city/NN addict/NN !/.\n",
      "who/WP can/MD get/VB a/DT pass/NNS in/IN ECons/NNS by/IN playing/VBG it/PRP ./.\n",
      "haha/NN I/PRP know/VB exactly/RB how/WRB you/PRP '/POS ll/NN look/VB at/IN me/PRP ./. stop/VB hahaha/NN ILY/NN !/.\n",
      "<A=4>\n",
      "minapaige/NN hi/NN mina/NN ,/, how/WRB are/VBP you/PRP ?/.\n",
      "Have/VBP not/RB heard/VBN from/IN you/PRP for/IN a/DT long/JJ time/NN ./.\n",
      "<A=4>\n",
      "MariahCarey/NN I/PRP want/VBP to/TO let/VB u/PRP kno/NN that/IN u/PRP are/VBP my/PRP$ fav/NN artist/NN of/IN all/DT time/NN ./.\n",
      "i/NN truly/RB admire/VB u/PRP and/CC wish/VBP u/PRP the/DT best/JJS .../: hope/NN to/TO hear/VB from/IN u/PRP !!/NN\n",
      "<A=4>\n",
      "eatingg/NN yummy/JJ lindtt/NN chocolatee/NN !/.\n",
      "woohoo/NN !/.\n",
      "<A=4>\n",
      "hi/NN RyanSeacrest/NN !/.\n",
      "I/PRP would/MD suggest/VBP music/NN of/IN my/PRP$ electro/NN project/NN ***/NN /NN ***/NN fresh/JJ ,/, kickin/NN '/POS ,/, different/JJ !/.\n",
      "free/JJ dl/NN &/CC have/VBP fun/NN\n",
      "<A=4>\n",
      "KikkerToo/NN Ford/NNP Focus/NNP CC/NN -/: 3/NN ./.\n",
      "Bloody/NNP excellent/JJ car/NN -/: just/RB needs/VBZ a/DT space/NN saver/NN wheel/NN in/IN the/DT boot/NN\n",
      "<A=4>\n",
      "physicsphaery/NN No/DT I/PRP didn/VBD 't/NN get/VB it/PRP yet/RB ,/, and/CC you/PRP confirmed/VBD my/PRP$ suspicions/NNS ./.\n",
      "And/CC gave/VBD me/PRP more/JJR !/.\n",
      "Thank/VB you/PRP !/.\n",
      "<A=4>\n",
      "MooMoo/NN _/NN 82/NN hehehe/NN v/NN ./. true/JJ ,/, if/IN only/RB it/PRP sent/VBD sms/NNS 's/POS to/TO our/PRP$ mobiles/NNS (/( like/IN in/IN the/DT US/PRP //NN UK/NNP )/) then/RB would/MD be/VB easier/JJR to/TO follow/VB =/SYM //NN Im/NN good/JJ ,/, studyin/NN .../:\n",
      "<A=4>\n",
      "Blogged/VBN our/PRP$ new/JJ offer/NN\n",
      "<A=4>\n",
      "i/NN neeeeeeeed/VBN to/TO peeeeeeee/NN\n",
      "<A=4>\n",
      "theitalianjob/NN :/: az/NN se/FW rossz/NN\n",
      "<A=4>\n",
      "is/VBZ bored/VBN as/IN CRAP/NN ./.\n",
      "Entertain/VB !!!!!/NN\n",
      "<A=4>\n",
      "Col/NN _/NN RFTL/NN thank/VB you/PRP honey/NN !/.\n",
      "I/PRP can/MD play/VB it/PRP again/RB now/RB do/VBP I/PRP refer/VB to/TO you/PRP as/IN Gandalf/NNP from/IN now/RB on/IN ;/: -/: )/)\n",
      "<A=4>\n",
      "Smile/NN ,/, and/CC the/DT world/NN will/MD smile/NN back/RB to/TO you/PRP\n",
      "<A=4>\n",
      "atebits/NNS it/PRP 's/POS 6pm/NN here/RB ,/, no/DT need/VBN for/IN sleep/VB yet/RB\n",
      "<A=4>\n",
      "Just/RB back/RB from/IN walking/VBG the/DT dog/NN ,/, catching/VBG up/IN on/IN emails/NNS and/CC stuff/NN after/IN a/DT weekend/NN gardening/VBG\n",
      "<A=4>\n",
      "filter/NN that/IN baby/NN bump/VB that/IN track/NN\n",
      "<A=4>\n",
      "Bored/VBN of/IN Victoria/NNP ,/, booking/VBG flights/NNS &/CC hotel/NN to/TO party/NN at/IN Seattle/NNP again/RB\n",
      "<A=4>\n",
      "yen/NNS _/NN menthol/NN Tell/VB me/PRP about/IN it/PRP ./.\n",
      "Have/VBP a/DT goodnight/NN\n",
      "<A=4>\n",
      "NovaWildstar/NN that/IN 's/POS good/JJ !/.\n",
      "I/PRP 'm/VBP not/RB properly/RB awake/JJ yet/RB and/CC I/PRP got/VBD up/IN at/IN 6am/NN ./.\n",
      "<A=4>\n",
      "Themeeee/NN park/NN ---/NN Roller/NNP coasters/NNS -/: loveee/NN\n",
      "<A=4>\n",
      "Senilius/NNS _/NN 110/NN Oh/UH ,/, enjoy/VB your/PRP$ new/JJ job/NN ./.\n",
      "Good/JJ luck/NN\n",
      "<A=4>\n",
      "Rachealblack110/NN I/PRP 'm/VBP great/JJ sweetie/NN How/WRB are/VBP you/PRP ?/.\n",
      "x/NN\n",
      "<A=4>\n",
      "evybabee/NN you/PRP look/VB like/IN you/PRP can/MD play/VB a/DT mean/NN guitar/NN too/RB and/CC cards/NNS ~/NN scott/NN\n",
      "<A=4>\n",
      "at/IN robclark182/NN\n",
      "<A=4>\n",
      "Rt/NN Birgit/NNP _/NN joenoia/NN douche/NN in/IN my/PRP$ native/JJ language/NN means/VBZ \"/\" shower/NN \"/\"\n",
      "<A=4>\n",
      "tatut/NN It/PRP was/VBD my/PRP$ mum/JJ who/WP got/VBD the/DT tickets/NNS\n",
      "<A=4>\n",
      "oliyoung/NN radiostarelle/NN herebeforeoprah/NN haha/NN good/JJ one/CD\n",
      "<A=4>\n",
      "levimorales/NNS Yeah/UH now/RB the/DT sun/NN really/RB does/VBZ shine/NN outta/IN my/PRP$ ./.\n",
      "Profile/NN ../.\n",
      "Got/VBD sick/JJ of/IN the/DT lawn/NN look/VB ,/, thought/VBD i/NN 'd/MD make/VB it/PRP look/VB a/DT little/JJ better/JJR\n",
      "<A=4>\n",
      "WomenCan/NN gotta/VB love/NN twitter/NN !/.\n",
      "i/NN learn/VB something/VBG new/JJ everyday/JJ\n",
      "<A=4>\n",
      "What/WP Cock/NN you/PRP dnt/NN have/VBP one/CD lil/NN boy/NN and/CC esides/NNS i/NN bet/NN jessica/NN will/MD .../:\n",
      "NOT/RB cuz/NN you/PRP '/POS re/NN like/IN hel/NN rank/NN and/CC all/DT\n",
      "<A=4>\n",
      "Had/VBD such/JJ a/DT relaxing/VBG weekend/NN &/CC am/VBP ready/JJ for/IN the/DT busy/JJ week/NN of/IN designing/VBG that/IN I/PRP have/VBP ahead/RB of/IN me/PRP\n",
      "<A=4>\n",
      "Umbrella/NN _/NN Skies/NNPS i/NN hate/VBP you/PRP rn/NN !/.\n",
      "only/RB not/RB really/RB .../: ILY/NN </SYM 3/NN but/CC i/NN am/VBP very/RB jealous/JJ !/.\n",
      "lol/NN ,/, have/VBP fun/NN\n",
      "<A=4>\n",
      "jacivelasquez/NN quote/VB on/IN myspace/NN from/IN you/PRP silly/RB ../.\n",
      "\"/\" Jaci/NN VelasquezFree/NN download/NN available/JJ \"/\" one/CD prob/NN no/DT link/NN ?????/NN\n",
      "Joy/NNP williams/NNS links/NNS !!!/NN\n",
      "<A=4>\n",
      "OMGoshness/NNS !!/NN\n",
      "britney/NN spears/NNS is/VBZ following/VBG me/PRP !!/NN\n",
      "lol/NN thankz/NN britz/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print output_buf.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_parser_output = \"\"\"<A=4>\n",
    "Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
    "Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<A=4>\\nMeet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\\nWear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\""
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_feature_vector(sentences, label):\n",
    "    if len(sentences) == 0:\n",
    "        return\n",
    "\n",
    "    feature_vector = []\n",
    "    function_set = [\n",
    "                    first_person_pronouns, \n",
    "                    second_person_pronouns, \n",
    "                    third_person_pronouns,\n",
    "                    coordinating_conjunctions,\n",
    "                    past_tense_verbs,\n",
    "                    future_tense_verbs,\n",
    "                    commas,\n",
    "                    colons,\n",
    "                    dashes,\n",
    "                    parantheses,\n",
    "                    ellipses,\n",
    "                    common_nouns,\n",
    "                    proper_nouns,\n",
    "                    adverbs,\n",
    "                    wh_words,\n",
    "                    slang_acronyms,\n",
    "                    upper_case_words,\n",
    "                    sentence_length,\n",
    "                    token_length,\n",
    "                    number_sentences\n",
    "                   ]\n",
    "    \n",
    "    for function in function_set:\n",
    "        feature_vector.append(function(sentences))\n",
    "        \n",
    "    feature_vector.append(label)\n",
    "    \n",
    "    print feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "sentence_container = []\n",
    "\n",
    "for line in sample_parser_output.split('\\n'):\n",
    "\n",
    "    if line.startswith('<A='):\n",
    "        class_label = int(line[3])\n",
    "        compute_feature_vector(sentence_container, class_label)\n",
    "        sentence_container = []\n",
    "    else:\n",
    "        sentence_container.append(line)\n",
    "    \n",
    "compute_feature_vector(sentence_container, class_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per-feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.',\n",
       " \"Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\"Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\", \"Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"]\n",
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentences(sentences):\n",
    "    tokens = [x.split(' ') for x in sentences]\n",
    "    return [y.split('/') for x in tokens for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Meet', 'VB'],\n",
       " ['me', 'PRP'],\n",
       " ['today', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['FEC', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['DC', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['4', 'NN'],\n",
       " ['.', '.'],\n",
       " ['Wear', 'VB'],\n",
       " ['a', 'DT'],\n",
       " ['carnation', 'NN'],\n",
       " ['so', 'RB'],\n",
       " ['I', 'PRP'],\n",
       " ['know', 'VB'],\n",
       " ['it', 'PRP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['you', 'PRP'],\n",
       " ['.', '.']]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['I', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['you', 'your', 'yours', 'u', 'ur', 'urs']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def third_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coordinating_conjunctions(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['CC']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinating_conjunctions(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def past_tense_verbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['VBD']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense_verbs(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def future_tense_verbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [\"'ll\", 'will', 'gonna']\n",
    "    count = [x[0] in candidate_words for x in token_split].count(True)\n",
    "    \n",
    "    # We also want to count sequences of going+to+VB\n",
    "    count += [token_split[i][0] == 'going' and token_split[i + 1][0] == 'to' and token_split[i + 2][1] == 'VB' for i in range(len(token_split) - 2)].count(True)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_tense_verbs(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commas(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [',']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commas(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colons(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [':', ';']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colons(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dashes(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['-']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parantheses(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['(', ')']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ellipses(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['...']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_nouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['NN', 'NNS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proper_nouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['NNP', 'NNPS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adverbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['RB', 'RBR', 'RBS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wh_words(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['WDT', 'WP', 'WP$', 'WRB']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slang_acronyms(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['smh', 'fwb',  'lmfao', 'lmao', 'lms', 'tbh',  'rofl', 'wtf',\n",
    "                       'bff', 'wyd',  'lylc',  'brb',  'atm', 'imao', 'sml',  'btw',\n",
    "                       'bw',  'imho', 'fyi',   'ppl',  'sob', 'ttyl', 'imo',  'ltr',\n",
    "                       'thx', 'kk',   'omg',   'ttys', 'afn', 'bbs',  'cya',  'ez',\n",
    "                       'f2f', 'gtr',  'ic',    'jk',   'k',   'ly',   'ya',   'nm',  'np',\n",
    "                       'plz', 'ru',   'so',    'tc',   'tmi', 'ym',   'ur',   'u',   'sol']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_acronyms(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upper_case_words(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    return [x[0].isupper() and len(x[0]) > 1 for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_case_words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_length(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    return len(token_split) / float(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.5"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_length(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['#', '$', '.', ',', ':', '(', ')', '\"', 'POS']\n",
    "    token_lengths = [len(x[0]) for x in token_split if x[1] not in candidate_words]\n",
    "    return sum(token_lengths) / float(len(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.888888888888889"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_sentences(sentences):\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Watson NLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"credentials\": {\n",
    "    \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api\",\n",
    "    \"username\": \"2bd0e6c7-5784-4967-860c-a9778754fdee\",\n",
    "    \"password\": \"rFs4Solusscl\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
