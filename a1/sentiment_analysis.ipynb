{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import NLPlib as nlp\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import HTMLParser\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Pre-process, tokenize and tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CSV Format:\n",
    "\n",
    "1. the polarity of the tweet (0 = negative emotion, 4 = positive emotion)\n",
    "2. the id of the tweet (e.g., 2087)\n",
    "3. the date of the tweet (e.g., Sat May 16 23:58:44 UTC 2009)\n",
    "4. the query (e.g., lyx). If there is no query, then this value is NO QUERY. \n",
    "5. the user that tweeted (e.g., robotickilldozr)\n",
    "6. the text of the tweet (e.g., Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GID = 4\n",
    "class_zero_data = [GID * 5500, GID * 5500 + 10] # (GID + 1) * 5500 - 1]\n",
    "class_four_data = [GID * 5500 + 800000, GID * 5500 + 800000 + 10] # (GID + 1) * 5500 - 1 + 800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1557393927', 'Sun Apr 19 03:50:43 PDT 2009', 'NO_QUERY', 'davidgarlick', '@Natalia_Bella not much to buy now Woolworth closed down ']\n",
      "['0', '1557394133', 'Sun Apr 19 03:50:48 PDT 2009', 'NO_QUERY', 'sarah_etf', 'Kill me please -.- ...Oh crap school tommorow ']\n",
      "['0', '1557394170', 'Sun Apr 19 03:50:48 PDT 2009', 'NO_QUERY', 'hypnotic', \"@chriskeating re the labour general secretary meeting with Labour PM's aide - I posted the very same on facebook. BBC gone downhill \"]\n",
      "['0', '1557394563', 'Sun Apr 19 03:50:57 PDT 2009', 'NO_QUERY', 'Robbertt', 'Whole day of homework ahead of name ']\n",
      "['0', '1557394776', 'Sun Apr 19 03:51:02 PDT 2009', 'NO_QUERY', 'desii____8', \"hamlet...romeo n juliet...radio:ACTIVE live at Wembley...McFly tour DVD's too money to me \"]\n",
      "['0', '1557394848', 'Sun Apr 19 03:51:03 PDT 2009', 'NO_QUERY', 'loubeejones', '@charleypearson haha, lucky you. i just got told one!  loubee is not happy!']\n",
      "['0', '1557395142', 'Sun Apr 19 03:51:09 PDT 2009', 'NO_QUERY', 'musicjunkie92', '@SpecialEmily aw he says thank you! Yea its lush here got dress&amp;flipflops on but i broke my sunnies  gettin new ones @ the beach tho ']\n",
      "['0', '1557395351', 'Sun Apr 19 03:51:14 PDT 2009', 'NO_QUERY', 'Javamomma', '@Bruno108 Ohhh, so much? But I have?! Ohh, I feel terrible! ']\n",
      "['0', '1557395422', 'Sun Apr 19 03:51:16 PDT 2009', 'NO_QUERY', 'magnusholmgren', 'Inspiration is laughing at me ']\n",
      "['0', '1557395551', 'Sun Apr 19 03:51:19 PDT 2009', 'NO_QUERY', 'nammymoo', \"can't believe match was cancelled   is officially gutted. . .\"]\n",
      "['4', '1563974733', 'Mon Apr 20 01:08:17 PDT 2009', 'NO_QUERY', 'judysteapot', '@libbyoliver you are very sweet my profile pic has been brushed up or whatever the technical term is!!   my car is a Fiat Qubo']\n",
      "['4', '1563974783', 'Mon Apr 20 01:08:19 PDT 2009', 'NO_QUERY', 'tonisep', '@trinta I switched powerpoint to keynote few years ago and making presentations got much easier ']\n",
      "['4', '1563974787', 'Mon Apr 20 01:08:18 PDT 2009', 'NO_QUERY', 'HelenTWilliams', '4km technique swim set done. Meeting with a creative director at 10am. Photographs to the printers. A million phone calls to make ']\n",
      "['4', '1563974863', 'Mon Apr 20 01:08:20 PDT 2009', 'NO_QUERY', 'alejandralei', 'I turn 17 in 17 days.!  now I am really going to sleep after watching 17 Again (:']\n",
      "['4', '1563974933', 'Mon Apr 20 01:08:21 PDT 2009', 'NO_QUERY', 'elenamarisol', 'I have to addmitt the movie was 3 stars and the book was 100000 stars ']\n",
      "['4', '1563974951', 'Mon Apr 20 01:08:21 PDT 2009', 'NO_QUERY', 'c0decafe', '@eMxyzptlk just meant &quot;eMxyzptlk&quot;, random rotted b64? j/k ']\n",
      "['4', '1563974997', 'Mon Apr 20 01:08:25 PDT 2009', 'NO_QUERY', 'Mattblahh', 'Camping was cool. Plan of attack for today; scrubs season 5, beginning to end. Beats college ']\n",
      "['4', '1563975055', 'Mon Apr 20 01:08:23 PDT 2009', 'NO_QUERY', 'apearlGirl', '@KatieVanBeek Me too! I wanted to be there all during break and this weather is soo perfect! ']\n",
      "['4', '1563975104', 'Mon Apr 20 01:08:24 PDT 2009', 'NO_QUERY', 'Jaegerbaby', \"Woooohooo! Finally getting some time off after working 9 weeks solid without a day off. Ok one day off for easter Sunday butthat's all! \"]\n",
      "['4', '1563975196', 'Mon Apr 20 01:08:26 PDT 2009', 'NO_QUERY', 'dleth', 'working on different politic media plans for the upcoming EU elections... ']\n"
     ]
    }
   ],
   "source": [
    "with open('training.1600000.processed.noemoticon.csv', 'rb') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    \n",
    "    for row in itertools.islice(reader, *class_zero_data):\n",
    "        print row\n",
    "    for row in itertools.islice(reader, *class_four_data):\n",
    "        print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working on different politic media plans for the upcoming EU elections... '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. All html tags and attributes (i.e., /<[^>]+>/) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(tweet):\n",
    "    regex = re.compile('<[^>]+>')\n",
    "    return regex.sub('', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Want This text!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_html_tags('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Html character codes (i.e., &...;) are replaced with an ASCII equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_html_codes(tweet):\n",
    "    parser = HTMLParser.HTMLParser()\n",
    "    return parser.unescape(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You win ยฃ100 ยก \"\n"
     ]
    }
   ],
   "source": [
    "print replace_html_codes('&quot;You win &pound;100 &iexcl; &quot;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. All URLs (i.e., tokens beginning with http or www) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_urls(tweet):\n",
    "    # Note that this will modify the whitespace when words are separated by\n",
    "    # more than one space, but that shouldn't matter as we are tokenizing\n",
    "    # the tweets anyways\n",
    "    \n",
    "    return ' '.join(filter(lambda x : not x.startswith(('www', 'http')), tweet.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brad is the best dancer but not the worst singer'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(\"brad is the best www.youtube.com dancer but not the worst http://www.google.ca singer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The first character in Twitter user names (@) and hash tags (#) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet):\n",
    "    return ' '.join([ x[1:] if  x.startswith(('@', '#')) else x for x in tweet.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brad donkey kick face @other #test'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hashtags('brad #donkey @kick face #@other @#test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Each sentence within a tweet is on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_abbrev_set(file_path='abbrev.english'):\n",
    "    abbrev_set = set()\n",
    "    \n",
    "    with open(file_path, 'rb') as abbrevs:\n",
    "        for line in abbrevs:\n",
    "            abbrev_set.union(line.strip())\n",
    "            #print line.strip()\n",
    "    \n",
    "    return abbrev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abbrev_set = create_abbrev_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print abbrev_set.issubset('Mr.')\n",
    "print abbrev_set.issubset('vs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_by_sentence(tweet):\n",
    "    '''\n",
    "        # 1. Anything ending in .?! declared a sentence\n",
    "        # 2. Sentence boundary moved after quotation mark, if any ex. He said, \"I am coming.\"\n",
    "        # 3. Period boundary is disqualified if it preceded by an element in abbrev_set\n",
    "        #    <We could look for capitals after an EOS, but nobody uses capitals on twitter>\n",
    "        #    <Both sides of :;- could also be thought of as sentence>\n",
    "    '''\n",
    "    \n",
    "    abbrev_set = create_abbrev_set()\n",
    "    split_by_space = tweet.split(' ')\n",
    "    eos_indices = [i + 1 for i, x in enumerate(split_by_space) if x[-1] in {'.', '?', '!'} or x[-2:-1] in {'.\"', '?\"', '!\"'} and not abbrev_set.issubset(x)]\n",
    "    return [' '.join(x) for x in [split_by_space[i:j] for i, j in zip([0] + eos_indices, eos_indices + [None])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_tweet = '4km technique swim set done. Meeting with a creative director at 10am. Photographs to the printers. A million phone calls to make'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4km technique swim set done.',\n",
       " 'Meeting with a creative director at 10am.',\n",
       " 'Photographs to the printers.',\n",
       " 'A million phone calls to make']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_sentence(sample_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Each token is tagged with its part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp.NLPlib()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'PRP']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [\"tag\", \"me\"]\n",
    "tags = tagger.tag(sent)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag('\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_parser_output = \"\"\"<A=4>\n",
    "Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
    "Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<A=4>\\nMeet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\\nWear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\""
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_feature_vector(sentences, label):\n",
    "    if len(sentences) == 0:\n",
    "        return\n",
    "\n",
    "    feature_vector = []\n",
    "    function_set = [\n",
    "                    first_person_pronouns, \n",
    "                    second_person_pronouns, \n",
    "                    third_person_pronouns,\n",
    "                    coordinating_conjunctions,\n",
    "                    past_tense_verbs,\n",
    "                    future_tense_verbs,\n",
    "                    commas,\n",
    "                    colons,\n",
    "                    dashes,\n",
    "                    parantheses,\n",
    "                    ellipses,\n",
    "                    common_nouns,\n",
    "                    proper_nouns,\n",
    "                    adverbs,\n",
    "                    wh_words,\n",
    "                    slang_acronyms,\n",
    "                    upper_case_words,\n",
    "                    sentence_length,\n",
    "                    token_length,\n",
    "                    number_sentences\n",
    "                   ]\n",
    "    \n",
    "    for function in function_set:\n",
    "        feature_vector.append(function(sentences))\n",
    "        \n",
    "    feature_vector.append(label)\n",
    "    \n",
    "    print feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "sentence_container = []\n",
    "\n",
    "for line in sample_parser_output.split('\\n'):\n",
    "\n",
    "    if line.startswith('<A='):\n",
    "        class_label = int(line[3])\n",
    "        compute_feature_vector(sentence_container, class_label)\n",
    "        sentence_container = []\n",
    "    else:\n",
    "        sentence_container.append(line)\n",
    "    \n",
    "compute_feature_vector(sentence_container, class_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per-feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.',\n",
       " \"Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\"Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\", \"Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"]\n",
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentences(sentences):\n",
    "    tokens = [x.split(' ') for x in sentences]\n",
    "    return [y.split('/') for x in tokens for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Meet', 'VB'],\n",
       " ['me', 'PRP'],\n",
       " ['today', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['FEC', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['DC', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['4', 'NN'],\n",
       " ['.', '.'],\n",
       " ['Wear', 'VB'],\n",
       " ['a', 'DT'],\n",
       " ['carnation', 'NN'],\n",
       " ['so', 'RB'],\n",
       " ['I', 'PRP'],\n",
       " ['know', 'VB'],\n",
       " ['it', 'PRP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['you', 'PRP'],\n",
       " ['.', '.']]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['I', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['you', 'your', 'yours', 'u', 'ur', 'urs']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def third_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coordinating_conjunctions(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['CC']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinating_conjunctions(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def past_tense_verbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['VBD']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense_verbs(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def future_tense_verbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [',']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commas(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [',']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commas(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colons(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [':', ';']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colons(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dashes(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['-']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parantheses(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['(', ')']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ellipses(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['...']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_nouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['NN', 'NNS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proper_nouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['NNP', 'NNPS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adverbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['RB', 'RBR', 'RBS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wh_words(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['WDT', 'WP', 'WP$', 'WRB']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slang_acronyms(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['smh', 'fwb',  'lmfao', 'lmao', 'lms', 'tbh',  'rofl', 'wtf',\n",
    "                       'bff', 'wyd',  'lylc',  'brb',  'atm', 'imao', 'sml',  'btw',\n",
    "                       'bw',  'imho', 'fyi',   'ppl',  'sob', 'ttyl', 'imo',  'ltr',\n",
    "                       'thx', 'kk',   'omg',   'ttys', 'afn', 'bbs',  'cya',  'ez',\n",
    "                       'f2f', 'gtr',  'ic',    'jk',   'k',   'ly',   'ya',   'nm',  'np',\n",
    "                       'plz', 'ru',   'so',    'tc',   'tmi', 'ym',   'ur',   'u',   'sol']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_acronyms(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upper_case_words(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    return [x[0].isupper() and len(x[0]) > 1 for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_case_words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_length(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    return len(token_split) / float(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.5"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_length(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['#', '$', '.', ',', ':', '(', ')', '\"', 'POS']\n",
    "    token_lengths = [len(x[0]) for x in token_split if x[1] not in candidate_words]\n",
    "    return sum(token_lengths) / float(len(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.888888888888889"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_sentences(sentences):\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Watson NLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"credentials\": {\n",
    "    \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api\",\n",
    "    \"username\": \"2bd0e6c7-5784-4967-860c-a9778754fdee\",\n",
    "    \"password\": \"rFs4Solusscl\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
