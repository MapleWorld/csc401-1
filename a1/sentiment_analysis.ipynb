{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import NLPlib as nlp\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import HTMLParser\n",
    "\n",
    "import StringIO\n",
    "import string\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Pre-process, tokenize and tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CSV Format:\n",
    "\n",
    "0. the polarity of the tweet (0 = negative emotion, 4 = positive emotion)\n",
    "1. the id of the tweet (e.g., 2087)\n",
    "2. the date of the tweet (e.g., Sat May 16 23:58:44 UTC 2009)\n",
    "3. the query (e.g., lyx). If there is no query, then this value is NO QUERY. \n",
    "4. the user that tweeted (e.g., robotickilldozr)\n",
    "5. the text of the tweet (e.g., Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GID = 4\n",
    "class_zero_data = [GID * 5500, GID * 5500 + 5] # (GID + 1) * 5500 - 1]\n",
    "class_four_data = [GID * 5500 + 800000, GID * 5500 + 800000 + 5] # (GID + 1) * 5500 - 1 + 800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_buf = StringIO.StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Natalia_Bella not much to buy now Woolworth closed down \n",
      "Kill me please -.- ...Oh crap school tommorow \n",
      "@chriskeating re the labour general secretary meeting with Labour PM's aide - I posted the very same on facebook. BBC gone downhill \n",
      "Whole day of homework ahead of name \n",
      "hamlet...romeo n juliet...radio:ACTIVE live at Wembley...McFly tour DVD's too money to me \n",
      "staying at home like the good girls do \n",
      "Morning world. It's a beautiful day  Here's hoping for some pathetic fallacy\n",
      "Oh, how I wish @johncmayer  would say hello to me on a tweet.  That man is a God in my eyes...and ugh, the body....okay I'm done \n",
      "Just woke up. Eating sandwiches and drinking coffee  Oh yeaa..\n",
      "@JimLundy  we have made it very easy for them to catch up  http://bit.ly/5TUpg\n"
     ]
    }
   ],
   "source": [
    "with open('training.1600000.processed.noemoticon.csv', 'rb') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    \n",
    "    for row in itertools.islice(reader, *class_zero_data):\n",
    "        print row[5]\n",
    "    for row in itertools.islice(reader, *class_four_data):\n",
    "        print row[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_buf = StringIO.StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for line in string_buf.getvalue().split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. All html tags and attributes (i.e., /<[^>]+>/) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(tweet):\n",
    "    return re.sub(r'<[^>]+>', '', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Want This text!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_html_tags('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Html character codes (i.e., &...;) are replaced with an ASCII equivalent.\n",
    "- Remove the ascii encoding to support extended in unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_html_codes(tweet):\n",
    "    parser = HTMLParser.HTMLParser()\n",
    "    tweet = filter(lambda x: x in string.printable, tweet)\n",
    "    return parser.unescape(tweet).encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You win 100  \" $\n"
     ]
    }
   ],
   "source": [
    "print replace_html_codes('&quot;You win &pound;100 &iexcl; &quot; &#36;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were back at work\n"
     ]
    }
   ],
   "source": [
    "print replace_html_codes(\"Weï¿½re back at work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. All URLs (i.e., tokens beginning with http or www) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_urls(tweet):\n",
    "    # Note that this will modify the whitespace when words are separated by\n",
    "    # more than one space, but that shouldn't matter as we are tokenizing\n",
    "    # the tweets anyways\n",
    "    \n",
    "    return ' '.join(filter(lambda x : not x.lower().startswith(('www', 'http')), tweet.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brad is the best dancer but not the worst singer'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(\"brad is the best www.youtube.com dancer but not the worst http://www.google.ca singer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The first character in Twitter user names (@) and hash tags (#) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet):\n",
    "    return ' '.join([ x[1:] if  x.startswith(('@', '#')) else x for x in tweet.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brad donkey kick face @other #test'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hashtags('brad #donkey @kick face #@other @#test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Each sentence within a tweet is on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_abbrev_set(file_path='Wordlists/abbrev.english'):\n",
    "    abbrev_set = set()\n",
    "    \n",
    "    with open(file_path, 'rb') as abbrevs:\n",
    "        for line in abbrevs:\n",
    "            abbrev_set.add(line.strip())\n",
    "            #print line.strip()\n",
    "    \n",
    "    return abbrev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abbrev_set = create_abbrev_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print 'Mr.' in abbrev_set\n",
    "print 'Pa.' in abbrev_set\n",
    "print 'Pr.' in abbrev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_by_sentence(tweet):\n",
    "    '''\n",
    "        # 1. Anything ending in .?! declared a sentence\n",
    "        # 2. Sentence boundary moved after quotation mark, if any ex. He said, \"I am coming.\"\n",
    "        # 3. Period boundary is disqualified if it preceded by an element in abbrev_set\n",
    "        #    <We could look for capitals after an EOS, but nobody uses capitals on twitter>\n",
    "        #    <Both sides of :;- could also be thought of as sentence>\n",
    "    '''\n",
    "    \n",
    "    tweet = re.sub(r' +', ' ', tweet).strip()\n",
    "    abbrev_set = create_abbrev_set()\n",
    "    split_by_space = tweet.split(' ')\n",
    "\n",
    "    quote_eos = lambda x: len(x) > 1 and (x[-2:] in {'.\"', '?\"', '!\"'} or x[-2:] in {\".'\", \"?'\", \"!'\"})\n",
    "    eos = lambda x: (x[-1] in {'.', '?', '!'} and x not in abbrev_set) or (quote_eos(x) and x[:-1] not in abbrev_set)\n",
    "    eos_indices = [i + 1 for i, x in enumerate(split_by_space) if eos(x)]\n",
    "    \n",
    "    if 0 == len(eos_indices):\n",
    "        return [tweet]\n",
    "    \n",
    "    sents = [' '.join(x) for x in [split_by_space[i:j] for i, j in zip([0] + eos_indices[:-1], eos_indices)]]\n",
    "\n",
    "    if eos_indices[-1] < len(split_by_space):\n",
    "        sents = sents + [' '.join(split_by_space[eos_indices[-1]:])]\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_tweet = '4km technique swim set done Mr.\" Meeting with a creative director at 10am.\" Photographs to the printers. A million phone calls to make.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4km technique swim set done Mr.\" Meeting with a creative director at 10am.\"',\n",
       " 'Photographs to the printers.',\n",
       " 'A million phone calls to make.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_sentence(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meech13 damn it!!!!, \\xc2\\xa1 half. Mr. Mr.' way text! through the interview @the sound cut's out, just' as u talk abt books!\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Meech13 damn it!!!!, Â¡ half. Mr. Mr.' way text! through the interview @the sound cut's out, just' as u talk abt books!\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meech13 damn it!!!!, \\xc2\\xa1 half.',\n",
       " \"Mr. Mr.' way text!\",\n",
       " \"through the interview @the sound cut's out, just' as u talk abt books!\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = split_by_sentence(test)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brad is the best']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = split_by_sentence(\"brad is the best\")\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpecialEmily aw he says thank you! Yea its lush here got dress&flipflops on but i broke my sunnies gettin new ones  the beach tho'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = \"SpecialEmily aw he says thank you! Yea its lush here got dress&flipflops on but i broke my sunnies gettin new ones  the beach tho\"\n",
    "test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpecialEmily aw he says thank you!',\n",
       " 'Yea its lush here got dress&flipflops on but i broke my sunnies gettin new ones the beach tho']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_sentence(test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6/7. Each token, including punctuation and clitics, is separated by spaces.\n",
    "- Clitics: contracted forms of words, such as nât\n",
    "- 's on possessive (ie. Brad's) different from 's on clitics (ie. What's), but both separated\n",
    "- Must also separate possessive on plurals (ie. dogs ')\n",
    "- Ellipsis (i.e., â...â), and other kinds of multiple punctuation (e.g., â!!!â) are not split.\n",
    "- Don't split e.g. into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_tokens(sentence):\n",
    "    # 1. Split on all punctuation symbols, where a given symbol is repeated once or more\n",
    "    sentence_1 = re.sub(r\"(([\"+ string.punctuation + \"])\\\\2*)\", r\" \\1 \", sentence).strip()\n",
    "    sentence_1 = ' '.join(sentence_1.split('  '))\n",
    "    \n",
    "    # 2. Join clitics and contractions where ' occurs mid-word\n",
    "    sentence_2 = re.sub(r\"(') ([A-Za-z] )\", r\"\\1\\2\", sentence_1)\n",
    "    \n",
    "    # 3. Join e.g.\n",
    "    sentence_3 = re.sub(r\" e . g . \", r\" e.g. \", sentence_2)\n",
    "    \n",
    "    return sentence_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"... Brad's dog hasn't said that the cereal is the dogs' or anyone elses..., but, we know  e.g. better????\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"... Brad 's dog hasn 't said that the cereal is the dogs ' or anyone elses ... , but , we know e.g. better ????\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tokens(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Each token is tagged with its part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_sentence(sentence, pos_tagger):\n",
    "    '''\n",
    "    Assume sentence is already separated into tokens\n",
    "    '''\n",
    "    split = sentence.split(' ')\n",
    "    return ' '.join([x[0] + \"/\" + x[1] for x in zip(split, pos_tagger.tag(split))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp.NLPlib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = \"Meet me today at the FEC in DC at 4 .\"\n",
    "sentence2 = \"Wear a carnation so I know it 's you .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
      "Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\n"
     ]
    }
   ],
   "source": [
    "# Expected:\n",
    "# Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
    "# Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP âs/POS you/PRP ./.\n",
    "print tag_sentence(sentence1, tagger)\n",
    "print tag_sentence(sentence2, tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Before each tweet is demarcation A=# in <> which occurs on its own line, where # is the numeric class of the tweet (0, 2, or 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_class(sentences, class_):\n",
    "    prepend = \"<A={}>\".format(class_)\n",
    "    return [prepend] + sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<A=4>',\n",
       " 'Meet me today at the FEC in DC at 4 .',\n",
       " \"Wear a carnation so I know it 's you .\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_class([sentence1, sentence2], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweet = \"@Meech13 damn it!!!!, half. Mr. Mr.' way <b>text!</b> through http://www.google.ca #the      interview #@the sound cut's out, just' as&#36; u talk abt books!   \"\n",
    "test_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tweet, t_class, tagger):\n",
    "    tweet = re.sub(r' +', ' ', tweet).strip()\n",
    "    tweet = strip_html_tags(tweet)\n",
    "    tweet = replace_html_codes(tweet)\n",
    "    tweet = remove_urls(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "\n",
    "    sentences = split_by_sentence(tweet)\n",
    "    sentences = [split_tokens(sentence) for sentence in sentences]\n",
    "    sentences = [tag_sentence(sentence, tagger) for sentence in sentences]\n",
    "    sentences = add_class(sentences, t_class)\n",
    "    \n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp.NLPlib()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<A=0>',\n",
       " 'Meech13/NN damn/JJ it/PRP !!!!/NN ,/, half/NN ./.',\n",
       " \"Mr/NNP ./. Mr/NNP ./. '/POS way/NN text/NN !/.\",\n",
       " \"through/IN the/DT interview/NN @/IN the/DT sound/NN cut/VB 's/POS out/IN ,/, just/RB '/POS as/IN $/$ u/PRP talk/VB abt/NN books/NNS !/.\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test_tweet, test_class, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twtt(output_file, input_file='training.1600000.processed.noemoticon.csv', GID=4):\n",
    "    class_zero_data = [GID * 5500, (GID + 1) * 5500 - 1]\n",
    "    class_four_data = [GID * 5500 + 800000, (GID + 1) * 5500 - 1 + 800000]\n",
    "    \n",
    "    with open(input_file, 'r') as train_file:\n",
    "        reader = csv.reader(train_file)\n",
    "\n",
    "        tagger = nlp.NLPlib()\n",
    "        \n",
    "        for row in itertools.islice(reader, *class_zero_data):\n",
    "            tweet = row[5]\n",
    "            t_class = 0\n",
    "            sentences = preprocess(tweet, t_class, tagger)\n",
    "            for sentence in sentences:\n",
    "                output_file.write(sentence + '\\n')\n",
    "                \n",
    "        for row in itertools.islice(reader, *class_four_data):\n",
    "            tweet = row[5]\n",
    "            t_class = 4\n",
    "            sentences = preprocess(tweet, t_class, tagger)\n",
    "            for sentence in sentences:\n",
    "                output_file.write(sentence + '\\n')\n",
    "        \n",
    "        return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StringIO.StringIO instance at 0x10583fdd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_buf = StringIO.StringIO()\n",
    "twtt(output_buf, 'training.1600000.processed.noemoticon.csv', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<A=0>\n",
      "Natalia/NNP _/NN Bella/NNP not/RB much/JJ to/TO buy/VB now/RB Woolworth/NNP closed/VBD down/RB\n",
      "<A=0>\n",
      "Kill/VB me/PRP please/VB -/: ./. -/: /NN .../: Oh/UH crap/NN school/NN tommorow/NN\n",
      "<A=0>\n",
      "chriskeating/VBG re/NN the/DT labour/NN general/JJ secretary/NN meeting/VBG with/IN Labour/NNP PM/NNP 's/POS aide/NN -/: I/PRP posted/VBD the/DT very/RB same/JJ on/IN facebook/NN ./.\n",
      "BBC/NNP gone/VBN downhill/RB\n",
      "<A=0>\n",
      "Whole/JJ day/NN of/IN homework/NN ahead/RB of/IN name/NN\n",
      "<A=0>\n",
      "hamlet/NN .../: romeo/NN n/NN juliet/NN .../: radio/NN :/: ACTIVE/JJ live/VB at/IN Wembley/NN .../: McFly/RB tour/NN DVD/NN 's/POS too/RB money/NN to/TO me/PRP\n",
      "<A=0>\n",
      "charleypearson/NN haha/NN ,/, lucky/JJ you/PRP ./.\n",
      "i/NN just/RB got/VBD told/VBD one/CD !/.\n",
      "loubee/NN is/VBZ not/RB happy/JJ !/.\n",
      "<A=0>\n",
      "SpecialEmily/RB aw/UH he/PRP says/VBZ thank/VB you/PRP !/.\n",
      "Yea/UH its/PRP$ lush/JJ here/RB got/VBD dress/NNS &/CC flipflops/NNS on/IN but/CC i/NN broke/VBD my/PRP$ sunnies/NNS gettin/VBG new/JJ ones/NNS the/DT beach/NN tho/NN\n",
      "<A=0>\n",
      "Bruno108/NN Ohhh/NN ,/, so/RB much/JJ ?/.\n",
      "But/CC I/PRP have/VBP ?/. !/.\n",
      "Ohh/NN ,/, I/PRP feel/VB terrible/JJ !/.\n",
      "<A=0>\n",
      "Inspiration/NN is/VBZ laughing/VBG at/IN me/PRP\n",
      "<A=0>\n",
      "can/MD 't/NN believe/VBP match/VB was/VBD cancelled/VBN is/VBZ officially/RB gutted/VBN ./.\n",
      "./.\n",
      "./.\n",
      "<A=0>\n",
      "Is/VBZ sitting/VBG at/IN home/NN bored/VBN And/CC doesnt/NN want/VBP to/TO go/VB back/RB to/TO school/NN tomorrow/NN\n",
      "<A=0>\n",
      "Off/IN to/TO work/NN Yeah/UH ,/, ad/NN set/VBN Sunday/NNP !/.\n",
      "<A=0>\n",
      "100th/NN update/VB ./.\n",
      "im/NN at/IN uni/NN\n",
      "<A=0>\n",
      "has/VBZ been/VBN doin/VBG graphics/NNS work/NN for/IN aaaaaaaaaaaages/NNS now/RB x/NN\n",
      "<A=0>\n",
      "OH/NN !/.\n",
      "I/PRP '/POS ll/NN email/NN it/PRP to/TO you/PRP\n",
      "<A=0>\n",
      "GLaDOS/NN :/: one/CD of/IN the/DT most/RBS engaging/VBG characters/NNS ever/RB to/TO appear/VB in/IN a/DT videogame/NN ?/. ./.\n",
      "If/IN only/RB she/PRP hadn/NN 't/NN made/VBN me/PRP kill/VB my/PRP$ companion/NN cube/NN\n",
      "<A=0>\n",
      "Eating/VBG chocolate/NN and/CC back/RB to/TO my/PRP$ data/NNS warehouse/NN */SYM weeps/NNS */SYM\n",
      "<A=0>\n",
      "happy/JJ that/IN i/NN missed/VBD coronor/NN 's/POS bringin/NN out/IN a/DT nice/JJ old/JJ lady/NN 's/POS body/NN who/WP lived/VBD near/IN my/PRP$ dad/NN ./.\n",
      "she/PRP died/VBD all/DT on/IN her/PRP$ own/JJ\n",
      "<A=0>\n",
      "so/RB happy/JJ i/NN go/VB back/RB to/TO school/NN on/IN tuesday/NN not/RB tomorrow/NN ./.\n",
      "I/PRP MUST/MD GET/VB mileycyrus/NNS tickets/NNS\n",
      "<A=0>\n",
      "gomezzephyr/NN Ugh/UH sweetie/NN ,/, Ugh/UH\n",
      "<A=0>\n",
      "if/IN officially/RB single/JJ after/IN 4/NN ./. 5/NN years/NNS\n",
      "<A=0>\n",
      "tomorrow/NN school/NN again/RB .../:\n",
      "i/NN can/MD 'T/NN wait/VB so/RB see/VB you/PRP all/DT ;/: )/) .../:\n",
      "today/NN doing/VBG something/VBG for/IN school/NN ../.\n",
      "oh/UH i/NN hate/VBP it/PRP ../.\n",
      "<A=0>\n",
      "dittebb/NN pulled/VBD a/DT muscle/NN in/IN the/DT shoulder/NN ./.\n",
      "I/PRP sure/JJ hope/NN it/PRP gets/VBZ better/JJR soon/RB ./.\n",
      "<A=0>\n",
      "LyndaJWilson/NN STOP/VB IT/PRP !/.\n",
      "Right/RB now/RB ./.\n",
      "That/DT 's/POS an/DT order/NN xx/NN\n",
      "<A=0>\n",
      "boondocksaint1/NN Berlin/NNP got/VBD food/NN poisoning/VBG this/DT time/NN\n",
      "<A=0>\n",
      "ARGHHHH/NN .../: have/VBP misplaced/VBN my/PRP$ latest/JJS draft/NN of/IN the/DT comedy/NN show/NN Ive/NN been/VBN writing/VBG !!!!!!/NN\n",
      "Think/VBP I/PRP need/VBN a/DT break/NN\n",
      "<A=0>\n",
      "Preparing/VBG myself/PRP for/IN work/NN tomorrow/NN ./.\n",
      "People/NNS to/TO call/VB ,/, things/NNS to/TO organise/NN .../:\n",
      "1/NN day/NN without/IN work/NN does/VBZ not/RB count/NN as/IN a/DT weekend/NN\n",
      "<A=0>\n",
      "Winniex/NN No/DT time/NN right/NN now/RB im/NN packing/VBG in/IN 5/NN minutes/NNS roberts/NNS packing/VBG the/DT computer/NN so/RB no/DT computer/NN for/IN 1/NN month/NN\n",
      "<A=0>\n",
      "audiobullys/NNS Would/MD love/VB to/TO see/VB you/PRP when/WRB your/PRP$ in/IN Inverness/NNP But/CC not/RB over/IN 18/NN lol/NN oh/UH well/RB hopefuly/RB see/VB u/PRP some/DT other/JJ time/NN !/.\n",
      "<A=0>\n",
      "Meech13/NN damn/JJ it/PRP !/.\n",
      "half/NN way/NN through/IN the/DT interview/NN the/DT sound/NN cuts/NNS out/IN ,/, just/RB as/IN u/PRP talk/VB abt/NN books/NNS !/.\n",
      "<A=0>\n",
      "Oh/UH so/RB sad/JJ ,/, Leaky/JJ Lounge/NNP is/VBZ not/RB letting/VBG me/PRP in/IN\n",
      "<A=0>\n",
      "may/MD just/RB have/VBP a/DT sick/JJ house/NN ../. Youngest/JJS son/NN just/RB got/VBD up/IN &/CC says/VBZ \"/\" I/PRP 'm/VBP Sick/NNP !/. \"/\"\n",
      "<A=0>\n",
      "DRINKING/NN SHOTS/NNS AND/CC WATCHING/VBG INTERVENTION/NN IS/VBZ LIKE/IN LOOKING/VBG IN/IN A/DT MIRROR/NN AND/CC DRINKING/NN :/: 0/NN\n",
      "<A=0>\n",
      "no/DT time/NN to/TO twitter/NN\n",
      "<A=0>\n",
      "Am/NNP on/IN my/PRP$ own/JJ having/VBG been/VBN forced/VBN to/TO leave/VB last/JJ day/NN pizza/NN get/VB together/RB by/IN ridiculous/JJ phobia/NN of/IN birds/NNS ./.\n",
      "So/RB stressed/VBD out/IN\n",
      "<A=0>\n",
      "relieved/VBN to/TO no/DT longer/RB be/VB stuck/VBN in/IN the/DT tiny/JJ tiny/JJ elevator/NN at/IN 1am/NN !!!/NN\n",
      "Oh/UH my/PRP$ claustrophobia/NN\n",
      "<A=0>\n",
      "paulshadwell/NN Well/UH if/IN it/PRP makes/VBZ you/PRP feel/VB any/DT better/JJR ,/, we/PRP only/RB had/VBD Quorn/NN sausages/NNS The/DT rest/NN was/VBD nice/JJ though/IN !/.\n",
      "<A=0>\n",
      "jbekkema/NN Rain/NNP +/SYM balding/JJ tyres/NNS +/SYM being/VBG tired/VBN +/SYM not/RB really/RB paying/VBG attention/NN =/SYM hitting/VBG L/NNP plater/NN .../:\n",
      "His/PRP$ Mum/JJ was/VBD a/DT real/JJ bitch/NN too/RB\n",
      "<A=0>\n",
      "just/RB moved/VBD to/TO the/DT couch/NN cause/NN my/PRP$ boo/VB said/VBD i/NN was/VBD snoring/VBG\n",
      "<A=0>\n",
      "LaiRenee/NN Did/VBD you/PRP get/VB hit/VBD with/IN Mikey/NN ?/.\n",
      "<A=0>\n",
      "I/PRP am/VBP bored/VBN of/IN doing/VBG assignments/NNS\n",
      "<A=0>\n",
      "feeling/VBG pretty/RB ill/JJ today/NN just/RB wonna/NN go/VB bk/NN to/TO sleep/VB\n",
      "<A=0>\n",
      "McALiMeal/JJ You/PRP are/VBP never/RB going/VBG to/TO be/VB a/DT \"/\" fucked/VBN up/IN little/JJ kid/NN \"/\" ./.\n",
      "If/IN anyone/NN dared/VBD say/VBP that/IN to/TO you/PRP i/NN 'd/MD fucking/VBG punch/NN them/PRP D/NN :/: you/PRP '/POS re/NN amazing/JJ ./.\n",
      "<A=0>\n",
      "Kazzy1978/NN I/PRP didnt/NN think/VBP I/PRP drank/VBD that/IN much/JJ too/RB ./.\n",
      "Need/VB to/TO stop/VB drinking/VBG on/IN an/DT empty/JJ stomach/NN\n",
      "<A=0>\n",
      "Homework/NN BORING/JJ !!/NN\n",
      "<A=0>\n",
      "Troy/NNP killed/VBN the/DT cutest/JJS bird/NN ..../CD very/RB sad/JJ\n",
      "<A=0>\n",
      "I/PRP 'm/VBP really/RB not/RB looking/VBG forward/RB to/TO going/VBG into/IN work/NN today/NN Weather/NNP 's/POS too/RB nice/JJ outside/IN !/.\n",
      "<A=0>\n",
      "LoneStarshine/NN oh/UH no/DT !/.\n",
      "I/PRP 'm/VBP very/RB very/RB sorry/JJ to/TO hear/VB that/IN\n",
      "<A=0>\n",
      "NASA/NNP may/MD need/VBN extra/JJ $/$ 30b/NN to/TO stay/VB on/IN schedule/NN to/TO moon/NN (/( via/IN gaz4695/NN )/) What/WP prices/NNS Mars/NNP ??/NN\n",
      "<A=0>\n",
      "just/RB waiting/VBG on/IN the/DT netbbook/NN to/TO charge/NN up/IN then/RB a/DT lovely/RB day/NN at/IN the/DT office/NN ./.\n",
      "<A=4>\n",
      "finally/RB ,/, hand/NN in/IN date/NN for/IN an/DT assignment/NN ,/, everyone/NN is/VBZ online/JJ and/CC in/IN a/DT panic/NN lol/NN me/PRP toooooooo/NN\n",
      "<A=4>\n",
      "thechrisgriffin/NN I/PRP don/VB 't/NN know/VB .../: this/DT is/VBZ the/DT first/JJ I/PRP '/POS ve/NN heard/VBN of/IN Avril/NN ./.\n",
      "Are/VBP you/PRP secretly/RB a/DT sk8tr/NN boi/NN ?/.\n",
      "Or/CC .../: do/VBP you/PRP like/IN sk8tr/NN bois/FW ?/.\n",
      ";/: P/NN\n",
      "<A=4>\n",
      "Happy/NNP Monday/NNP everyone/NN Were/VBD back/RB at/IN work/NN !/.\n",
      "<A=4>\n",
      "chamada/NN Will/MD it/PRP be/VB your/PRP$ first/JJ time/NN to/TO Italy/RB ?/.\n",
      "You/PRP '/POS re/NN going/VBG to/TO love/NN it/PRP !/.\n",
      "My/PRP$ absolute/JJ favorite/JJ place/NN\n",
      "<A=4>\n",
      "Trying/VBG to/TO get/VB all/DT things/NNS together/RB for/IN my/PRP$ trip/NN to/TO Riga/NNP and/CC all/DT things/NNS needed/VBN for/IN immigration/NN purposes/NNS .../:\n",
      "<A=4>\n",
      "loves/VBZ Eastlink/NN .../:\n",
      "10/NN mins/NNS from/IN Blackburn/NNP Rd/NN to/TO Eastlink/NN on/IN High/NNP Street/NNP Rd/NN .../:\n",
      "3/NN mins/NNS to/TO Ringwood/NNP ./.\n",
      "<A=4>\n",
      "\"/\" Saw/VBD \"/\" The/DT game/NN is/VBZ scheduled/VBN to/TO release/NN in/IN 2009/NN\n",
      "<A=4>\n",
      "mezzle/NN Undisturbed/JJ 8/NN hour/NN sleep/VB ./.\n",
      "And/CC not/RB waking/VBG up/IN with/IN hangover/NN ==/NN good/JJ premise/NN for/IN the/DT day/NN\n",
      "<A=4>\n",
      "DocAdams/NNS well/RB I/PRP loved/VBD the/DT first/JJ so/RB I/PRP '/POS ll/NN jump/NN straight/JJ on/IN this/DT one/CD !/.\n",
      "<A=4>\n",
      "codelust/NN that/IN is/VBZ also/RB true/JJ\n",
      "<A=4>\n",
      "NatskiB/NN glad/JJ you/PRP like/IN !/.\n",
      "now/RB buy/VB some/DT\n",
      "<A=4>\n",
      "mattycus/NNS //NN hug/NN ./.\n",
      "Whatever/WDT you/PRP think/VBP is/VBZ best/JJS for/IN you/PRP\n",
      "<A=4>\n",
      "TheBigfella/NN no/DT tickets/NNS here/RB ./.\n",
      "we/PRP got/VBD ya/NN\n",
      "<A=4>\n",
      "Bye/UH guys/NNS ./.\n",
      "Gonna/VBG watch/VB my/PRP$ show/NN and/CC do/VBP my/PRP$ school/NN work/NN at/IN the/DT same/JJ time/NN !/.\n",
      "Multitasking/VBG !/.\n",
      "<A=4>\n",
      "bryantma/NN Not/RB a/DT bad/JJ start/VB ,/, but/CC don/VB 't/NN forget/VB that/IN my/PRP$ bruiser/NN 's/POS a/DT little/JJ older/JJR than/IN yours/PRP ;/: he/PRP 's/POS already/RB had/VBD his/PRP$ education/NN from/IN me/PRP\n",
      "<A=4>\n",
      "Sarr3o7/NN Restaurant/NNP city/NN addict/NN !/.\n",
      "who/WP can/MD get/VB a/DT pass/NNS in/IN ECons/NNS by/IN playing/VBG it/PRP ./.\n",
      "haha/NN I/PRP know/VB exactly/RB how/WRB you/PRP '/POS ll/NN look/VB at/IN me/PRP ./. stop/VB hahaha/NN ILY/NN !/.\n",
      "<A=4>\n",
      "minapaige/NN hi/NN mina/NN ,/, how/WRB are/VBP you/PRP ?/.\n",
      "Have/VBP not/RB heard/VBN from/IN you/PRP for/IN a/DT long/JJ time/NN ./.\n",
      "<A=4>\n",
      "MariahCarey/NN I/PRP want/VBP to/TO let/VB u/PRP kno/NN that/IN u/PRP are/VBP my/PRP$ fav/NN artist/NN of/IN all/DT time/NN ./.\n",
      "i/NN truly/RB admire/VB u/PRP and/CC wish/VBP u/PRP the/DT best/JJS .../: hope/NN to/TO hear/VB from/IN u/PRP !!/NN\n",
      "<A=4>\n",
      "eatingg/NN yummy/JJ lindtt/NN chocolatee/NN !/.\n",
      "woohoo/NN !/.\n",
      "<A=4>\n",
      "hi/NN RyanSeacrest/NN !/.\n",
      "I/PRP would/MD suggest/VBP music/NN of/IN my/PRP$ electro/NN project/NN ***/NN /NN ***/NN fresh/JJ ,/, kickin/NN '/POS ,/, different/JJ !/.\n",
      "free/JJ dl/NN &/CC have/VBP fun/NN\n",
      "<A=4>\n",
      "KikkerToo/NN Ford/NNP Focus/NNP CC/NN -/: 3/NN ./.\n",
      "Bloody/NNP excellent/JJ car/NN -/: just/RB needs/VBZ a/DT space/NN saver/NN wheel/NN in/IN the/DT boot/NN\n",
      "<A=4>\n",
      "physicsphaery/NN No/DT I/PRP didn/VBD 't/NN get/VB it/PRP yet/RB ,/, and/CC you/PRP confirmed/VBD my/PRP$ suspicions/NNS ./.\n",
      "And/CC gave/VBD me/PRP more/JJR !/.\n",
      "Thank/VB you/PRP !/.\n",
      "<A=4>\n",
      "MooMoo/NN _/NN 82/NN hehehe/NN v/NN ./. true/JJ ,/, if/IN only/RB it/PRP sent/VBD sms/NNS 's/POS to/TO our/PRP$ mobiles/NNS (/( like/IN in/IN the/DT US/PRP //NN UK/NNP )/) then/RB would/MD be/VB easier/JJR to/TO follow/VB =/SYM //NN Im/NN good/JJ ,/, studyin/NN .../:\n",
      "<A=4>\n",
      "Blogged/VBN our/PRP$ new/JJ offer/NN\n",
      "<A=4>\n",
      "i/NN neeeeeeeed/VBN to/TO peeeeeeee/NN\n",
      "<A=4>\n",
      "theitalianjob/NN :/: az/NN se/FW rossz/NN\n",
      "<A=4>\n",
      "is/VBZ bored/VBN as/IN CRAP/NN ./.\n",
      "Entertain/VB !!!!!/NN\n",
      "<A=4>\n",
      "Col/NN _/NN RFTL/NN thank/VB you/PRP honey/NN !/.\n",
      "I/PRP can/MD play/VB it/PRP again/RB now/RB do/VBP I/PRP refer/VB to/TO you/PRP as/IN Gandalf/NNP from/IN now/RB on/IN ;/: -/: )/)\n",
      "<A=4>\n",
      "Smile/NN ,/, and/CC the/DT world/NN will/MD smile/NN back/RB to/TO you/PRP\n",
      "<A=4>\n",
      "atebits/NNS it/PRP 's/POS 6pm/NN here/RB ,/, no/DT need/VBN for/IN sleep/VB yet/RB\n",
      "<A=4>\n",
      "Just/RB back/RB from/IN walking/VBG the/DT dog/NN ,/, catching/VBG up/IN on/IN emails/NNS and/CC stuff/NN after/IN a/DT weekend/NN gardening/VBG\n",
      "<A=4>\n",
      "filter/NN that/IN baby/NN bump/VB that/IN track/NN\n",
      "<A=4>\n",
      "Bored/VBN of/IN Victoria/NNP ,/, booking/VBG flights/NNS &/CC hotel/NN to/TO party/NN at/IN Seattle/NNP again/RB\n",
      "<A=4>\n",
      "yen/NNS _/NN menthol/NN Tell/VB me/PRP about/IN it/PRP ./.\n",
      "Have/VBP a/DT goodnight/NN\n",
      "<A=4>\n",
      "NovaWildstar/NN that/IN 's/POS good/JJ !/.\n",
      "I/PRP 'm/VBP not/RB properly/RB awake/JJ yet/RB and/CC I/PRP got/VBD up/IN at/IN 6am/NN ./.\n",
      "<A=4>\n",
      "Themeeee/NN park/NN ---/NN Roller/NNP coasters/NNS -/: loveee/NN\n",
      "<A=4>\n",
      "Senilius/NNS _/NN 110/NN Oh/UH ,/, enjoy/VB your/PRP$ new/JJ job/NN ./.\n",
      "Good/JJ luck/NN\n",
      "<A=4>\n",
      "Rachealblack110/NN I/PRP 'm/VBP great/JJ sweetie/NN How/WRB are/VBP you/PRP ?/.\n",
      "x/NN\n",
      "<A=4>\n",
      "evybabee/NN you/PRP look/VB like/IN you/PRP can/MD play/VB a/DT mean/NN guitar/NN too/RB and/CC cards/NNS ~/NN scott/NN\n",
      "<A=4>\n",
      "at/IN robclark182/NN\n",
      "<A=4>\n",
      "Rt/NN Birgit/NNP _/NN joenoia/NN douche/NN in/IN my/PRP$ native/JJ language/NN means/VBZ \"/\" shower/NN \"/\"\n",
      "<A=4>\n",
      "tatut/NN It/PRP was/VBD my/PRP$ mum/JJ who/WP got/VBD the/DT tickets/NNS\n",
      "<A=4>\n",
      "oliyoung/NN radiostarelle/NN herebeforeoprah/NN haha/NN good/JJ one/CD\n",
      "<A=4>\n",
      "levimorales/NNS Yeah/UH now/RB the/DT sun/NN really/RB does/VBZ shine/NN outta/IN my/PRP$ ./.\n",
      "Profile/NN ../.\n",
      "Got/VBD sick/JJ of/IN the/DT lawn/NN look/VB ,/, thought/VBD i/NN 'd/MD make/VB it/PRP look/VB a/DT little/JJ better/JJR\n",
      "<A=4>\n",
      "WomenCan/NN gotta/VB love/NN twitter/NN !/.\n",
      "i/NN learn/VB something/VBG new/JJ everyday/JJ\n",
      "<A=4>\n",
      "What/WP Cock/NN you/PRP dnt/NN have/VBP one/CD lil/NN boy/NN and/CC esides/NNS i/NN bet/NN jessica/NN will/MD .../:\n",
      "NOT/RB cuz/NN you/PRP '/POS re/NN like/IN hel/NN rank/NN and/CC all/DT\n",
      "<A=4>\n",
      "Had/VBD such/JJ a/DT relaxing/VBG weekend/NN &/CC am/VBP ready/JJ for/IN the/DT busy/JJ week/NN of/IN designing/VBG that/IN I/PRP have/VBP ahead/RB of/IN me/PRP\n",
      "<A=4>\n",
      "Umbrella/NN _/NN Skies/NNPS i/NN hate/VBP you/PRP rn/NN !/.\n",
      "only/RB not/RB really/RB .../: ILY/NN </SYM 3/NN but/CC i/NN am/VBP very/RB jealous/JJ !/.\n",
      "lol/NN ,/, have/VBP fun/NN\n",
      "<A=4>\n",
      "jacivelasquez/NN quote/VB on/IN myspace/NN from/IN you/PRP silly/RB ../.\n",
      "\"/\" Jaci/NN VelasquezFree/NN download/NN available/JJ \"/\" one/CD prob/NN no/DT link/NN ?????/NN\n",
      "Joy/NNP williams/NNS links/NNS !!!/NN\n",
      "<A=4>\n",
      "OMGoshness/NNS !!/NN\n",
      "britney/NN spears/NNS is/VBZ following/VBG me/PRP !!/NN\n",
      "lol/NN thankz/NN britz/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print output_buf.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lol/NN thankz/NN britz/NN', '']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_buf.getvalue().split('\\n')[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickle the dictionary\n",
      "Initialized lexHash from pickled data.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_files/train.twt\", \"w\") as train_file:\n",
    "    twtt(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "@relation weather\n",
    "\n",
    "@attribute outlook {sunny, overcast, rainy}\n",
    "\n",
    "@attribute temperature numeric\n",
    "\n",
    "@attribute humidity numeric\n",
    "\n",
    "@attribute windy {TRUE, FALSE}\n",
    "\n",
    "@attribute play {yes, no}\n",
    "\n",
    "@data\n",
    "sunny,85,85,FALSE,no\n",
    "sunny,80,90,TRUE,no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_parser_output = \"\"\"<A=4>\n",
    "Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\n",
    "Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<A=4>\\nMeet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\\nWear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\""
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_arff(output_file):\n",
    "    output_file.write(\"@relation sentiment\\n\\n\")\n",
    "    \n",
    "    feature_set = [\n",
    "                \"first_person_pronouns\", \n",
    "                \"second_person_pronouns\", \n",
    "                \"third_person_pronouns\",\n",
    "                \"coordinating_conjunctions\",\n",
    "                \"past_tense_verbs\",\n",
    "                \"future_tense_verbs\",\n",
    "                \"commas\",\n",
    "                \"colons\",\n",
    "                \"dashes\",\n",
    "                \"parantheses\",\n",
    "                \"ellipses\",\n",
    "                \"common_nouns\",\n",
    "                \"proper_nouns\",\n",
    "                \"adverbs\",\n",
    "                \"wh_words\",\n",
    "                \"slang_acronyms\",\n",
    "                \"upper_case_words\",\n",
    "                \"sentence_length\",\n",
    "                \"token_length\",\n",
    "                \"number_sentences\"\n",
    "               ]\n",
    "    \n",
    "    for feature in feature_set:\n",
    "        output_file.write(\"@attribute \" + feature + \" numeric\\n\")\n",
    "        \n",
    "    output_file.write(\"@attribute class {0, 4}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_feature_vector(sentences, label):\n",
    "    feature_string = \"\"\n",
    "    if 0 == len(sentences):\n",
    "        return feature_string\n",
    "    \n",
    "    function_set = [\n",
    "                    first_person_pronouns, \n",
    "                    second_person_pronouns, \n",
    "                    third_person_pronouns,\n",
    "                    coordinating_conjunctions,\n",
    "                    past_tense_verbs,\n",
    "                    future_tense_verbs,\n",
    "                    commas,\n",
    "                    colons,\n",
    "                    dashes,\n",
    "                    parantheses,\n",
    "                    ellipses,\n",
    "                    common_nouns,\n",
    "                    proper_nouns,\n",
    "                    adverbs,\n",
    "                    wh_words,\n",
    "                    slang_acronyms,\n",
    "                    upper_case_words,\n",
    "                    sentence_length,\n",
    "                    token_length,\n",
    "                    number_sentences\n",
    "                   ]\n",
    "    \n",
    "    for function in function_set:\n",
    "        feature_string += str(function(sentences)) + ','\n",
    "        \n",
    "    feature_string += str(label)\n",
    "    \n",
    "    return \"@data \" + feature_string + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def buildarff(input_file, output_file, max_per_class=50):\n",
    "\n",
    "    prep_arff(output_file)\n",
    "    \n",
    "    sentence_container = []\n",
    "\n",
    "    for line in input_file.getvalue().split('\\n')[:-1]:\n",
    "\n",
    "        if line.startswith('<A='):\n",
    "            class_label = int(line[3])\n",
    "            feature_vector = compute_feature_vector(sentence_container, class_label)\n",
    "            output_file.write(feature_vector)\n",
    "            sentence_container = []\n",
    "        else:\n",
    "            sentence_container.append(line)\n",
    "\n",
    "    compute_feature_vector(sentence_container, class_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arff_buf = StringIO.StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildarff(output_buf, arff_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@relation sentiment\n",
      "\n",
      "@attribute first_person_pronouns numeric\n",
      "@attribute second_person_pronouns numeric\n",
      "@attribute third_person_pronouns numeric\n",
      "@attribute coordinating_conjunctions numeric\n",
      "@attribute past_tense_verbs numeric\n",
      "@attribute future_tense_verbs numeric\n",
      "@attribute commas numeric\n",
      "@attribute colons numeric\n",
      "@attribute dashes numeric\n",
      "@attribute parantheses numeric\n",
      "@attribute ellipses numeric\n",
      "@attribute common_nouns numeric\n",
      "@attribute proper_nouns numeric\n",
      "@attribute adverbs numeric\n",
      "@attribute wh_words numeric\n",
      "@attribute slang_acronyms numeric\n",
      "@attribute upper_case_words numeric\n",
      "@attribute sentence_length numeric\n",
      "@attribute token_length numeric\n",
      "@attribute number_sentences numeric\n",
      "@attribute class {0, 4}\n",
      "\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,1,3,3,0,0,0,11.0,4.27272727273,1,0\n",
      "@data 1,0,0,0,0,0,0,0,2,0,1,4,0,0,0,0,0,12.0,4.0,1,0\n",
      "@data 0,0,0,0,1,0,0,0,1,0,0,5,3,2,0,0,2,12.0,5.0,2,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,3,0,1,0,0,0,7.0,4.14285714286,1,0\n",
      "@data 1,0,0,0,0,0,0,1,0,0,3,9,0,2,0,0,2,21.0,4.125,1,0\n",
      "@data 0,1,0,0,2,0,1,0,0,0,0,4,0,2,0,0,0,5.66666666667,4.38461538462,3,0\n",
      "@data 1,1,2,2,2,0,0,0,0,0,0,7,0,2,0,0,0,13.5,4.0,2,0\n",
      "@data 0,0,0,1,0,0,2,0,0,0,0,3,0,1,0,1,0,5.66666666667,3.81818181818,3,0\n",
      "@data 1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,5.0,5.0,1,0\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,4.0,5.22222222222,3,0\n",
      "@data 0,0,0,1,0,0,0,0,0,0,0,4,0,1,0,0,0,14.0,4.07142857143,1,0\n",
      "@data 0,0,0,0,0,0,1,0,0,0,0,2,1,0,0,0,0,9.0,3.42857142857,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,3.0,3.6,2,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,4,0,1,0,0,0,9.0,5.0,1,0\n",
      "@data 0,1,1,0,0,0,0,0,0,0,0,3,0,0,0,0,1,4.5,2.42857142857,2,0\n",
      "@data 2,0,1,0,0,0,0,1,0,0,0,7,0,3,0,0,0,13.5,4.16666666667,2,0\n",
      "@data 1,0,0,1,0,0,0,0,0,0,0,4,0,1,0,0,0,11.0,4.18181818182,1,0\n",
      "@data 1,0,2,0,3,0,0,0,0,0,0,6,0,0,1,0,0,13.0,3.65217391304,2,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,6,0,3,0,1,2,8.5,4.1875,2,0\n",
      "@data 0,0,0,0,0,0,1,0,0,0,0,2,0,0,0,0,0,5.0,6.0,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,3,0,1,0,0,0,8.0,4.28571428571,1,0\n",
      "@data 0,1,1,0,0,0,0,1,0,1,2,7,0,2,0,1,1,6.5,3.85,4,0\n",
      "@data 0,0,1,0,1,0,0,0,0,0,0,4,0,1,0,0,0,8.0,4.14285714286,2,0\n",
      "@data 0,0,1,0,0,0,0,0,0,0,0,3,0,2,0,0,2,4.0,4.33333333333,3,0\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,3,1,0,0,0,0,7.0,6.28571428571,1,0\n",
      "@data 1,0,0,0,0,0,0,0,0,0,1,7,0,0,0,0,1,10.0,4.42105263158,2,0\n",
      "@data 0,0,0,0,0,0,1,0,0,0,1,10,0,1,0,0,0,8.0,4.52380952381,3,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,11,0,2,0,1,0,20.0,4.3,1,0\n",
      "@data 0,3,0,1,0,0,0,0,0,0,0,4,1,3,1,1,0,24.0,4.04347826087,1,0\n",
      "@data 0,1,1,0,0,0,1,0,0,0,0,8,0,1,0,1,0,10.5,4.05555555556,2,0\n",
      "@data 1,0,0,0,0,0,1,0,0,0,0,0,1,2,0,1,0,11.0,3.4,1,0\n",
      "@data 0,0,0,1,1,0,0,0,0,0,0,2,1,2,0,0,0,20.0,3.3125,1,0\n",
      "@data 0,0,0,2,0,0,0,1,0,0,0,6,0,0,0,0,12,15.0,5.0,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,4.0,3.75,1,0\n",
      "@data 1,0,0,0,1,0,0,0,0,0,0,4,1,2,0,1,0,11.5,4.22727272727,2,0\n",
      "@data 1,0,0,0,0,0,0,0,0,0,0,4,0,1,0,0,0,8.5,4.23529411765,2,0\n",
      "@data 1,1,1,0,2,0,1,0,0,0,0,4,0,1,0,0,0,21.0,4.36842105263,1,0\n",
      "@data 0,0,1,0,1,0,0,0,0,0,1,5,2,3,0,0,0,12.5,4.08333333333,2,0\n",
      "@data 1,0,0,0,3,0,0,0,0,0,0,3,0,1,0,0,0,12.0,3.66666666667,1,0\n",
      "@data 0,1,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,8.0,4.14285714286,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,6.0,4.33333333333,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,3,0,2,0,0,0,10.0,4.1,1,0\n",
      "@data 0,3,1,0,1,1,0,1,0,0,0,6,0,1,0,0,0,17.0,3.71428571429,2,0\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,3,0,1,0,0,0,9.0,4.17647058824,2,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,1,3.0,5.33333333333,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,8.0,4.25,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,2,1,4,0,0,0,17.0,4.46666666667,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,1,0,2,0,0,0,6.0,3.90909090909,2,0\n",
      "@data 0,0,0,0,0,0,0,0,0,2,0,6,2,0,1,0,1,20.0,3.94117647059,1,0\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,4,0,3,0,0,0,16.0,3.93333333333,1,4\n",
      "@data 1,0,0,1,0,0,2,0,0,0,0,7,0,1,0,0,0,19.0,4.29411764706,1,4\n",
      "@data 0,2,0,1,0,0,0,1,0,0,2,8,0,1,0,0,0,8.5,3.62962962963,4,4\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,2,2,1,0,0,0,8.0,4.71428571429,1,4\n",
      "@data 1,2,2,0,0,1,0,0,0,0,0,5,0,1,0,0,0,7.33333333333,4.0,3,4\n",
      "@data 1,0,0,1,0,0,0,0,0,0,1,5,1,1,0,0,0,19.0,4.61111111111,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,2,8,4,0,0,0,0,6.66666666667,4.29411764706,3,4\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,3,0,0,0,0,0,11.0,4.0,1,4\n",
      "@data 0,0,0,1,0,0,0,0,0,0,0,7,0,1,0,0,0,9.0,4.41176470588,2,4\n",
      "@data 0,0,0,0,1,0,0,0,0,0,0,3,0,2,0,1,0,16.0,3.71428571429,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,5.0,4.4,1,4\n",
      "@data 0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,4.0,4.0,2,4\n",
      "@data 0,2,0,0,0,0,0,0,0,0,0,2,0,0,1,0,0,5.5,3.9,2,4\n",
      "@data 1,0,0,0,1,0,0,0,0,0,0,3,0,1,0,1,0,4.0,4.42857142857,2,4\n",
      "@data 2,0,0,1,0,1,0,0,0,0,0,5,0,0,0,0,0,6.33333333333,4.0625,3,4\n",
      "@data 2,1,2,1,1,0,1,1,0,0,0,4,0,2,0,0,0,28.0,4.08333333333,1,4\n",
      "@data 1,1,1,0,0,0,0,0,0,0,0,9,1,1,2,0,1,10.6666666667,3.85185185185,3,4\n",
      "@data 0,2,0,0,0,0,1,0,0,0,0,4,0,1,1,0,0,9.0,3.66666666667,2,4\n",
      "@data 1,5,0,1,0,0,0,0,0,0,1,8,0,1,0,5,0,16.5,3.16129032258,2,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,3.5,6.8,2,4\n",
      "@data 1,0,0,1,0,0,2,0,0,0,0,11,0,0,0,0,0,8.66666666667,4.28571428571,3,4\n",
      "@data 0,0,0,0,0,0,0,0,2,0,0,8,3,1,0,0,1,10.0,4.29411764706,2,4\n",
      "@data 2,2,1,2,3,0,1,0,0,0,0,3,0,1,0,0,0,7.66666666667,4.10526315789,3,4\n",
      "@data 2,0,1,0,1,0,2,0,0,2,1,9,1,2,0,0,2,38.0,3.1935483871,1,4\n",
      "@data 1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,4.0,4.5,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,4.0,5.5,1,4\n",
      "@data 0,0,0,0,0,0,0,1,0,0,0,3,0,0,0,0,0,5.0,5.5,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,1,3.5,4.5,2,4\n",
      "@data 0,2,1,0,0,0,0,1,1,1,0,4,1,3,0,0,1,13.0,3.18181818182,2,4\n",
      "@data 0,1,0,1,0,1,1,0,0,0,0,3,0,1,0,0,0,10.0,3.77777777778,1,4\n",
      "@data 0,0,1,0,0,0,1,0,0,0,0,2,0,2,0,0,0,11.0,3.66666666667,1,4\n",
      "@data 0,0,0,1,0,0,1,0,0,0,0,4,0,2,0,0,0,17.0,4.5625,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,6.0,4.5,1,4\n",
      "@data 0,0,0,1,0,0,1,0,0,0,0,3,2,1,0,0,0,13.0,4.66666666667,1,4\n",
      "@data 1,0,1,0,0,0,0,0,0,0,0,4,0,0,0,0,0,5.5,3.8,2,4\n",
      "@data 0,0,0,1,1,0,0,0,0,0,0,2,0,3,0,0,0,9.0,3.73333333333,2,4\n",
      "@data 0,0,0,0,0,0,0,0,1,0,0,5,1,0,0,0,0,7.0,5.83333333333,1,4\n",
      "@data 0,1,0,0,0,0,1,0,0,0,0,5,0,0,0,0,0,6.0,3.7,2,4\n",
      "@data 0,1,0,0,0,0,0,0,0,0,0,3,0,0,1,0,0,5.0,4.44444444444,2,4\n",
      "@data 0,2,0,1,0,0,0,0,0,0,0,6,0,1,0,0,0,15.0,3.8,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,2.0,6.5,1,4\n",
      "@data 1,0,0,0,0,0,0,0,0,0,0,6,1,0,0,0,0,13.0,4.63636363636,1,4\n",
      "@data 1,0,1,0,2,0,0,0,0,0,0,2,0,0,1,0,0,9.0,3.44444444444,1,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,6.0,7.83333333333,1,4\n",
      "@data 1,0,1,0,2,0,1,0,0,0,0,6,0,2,0,0,0,9.66666666667,4.07692307692,3,4\n",
      "@data 0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,5.0,5.55555555556,2,4\n",
      "@data 0,2,0,2,0,1,0,0,0,0,1,12,0,1,1,0,1,12.5,3.4347826087,2,4\n",
      "@data 1,0,0,1,1,0,0,0,0,0,0,2,0,1,0,0,0,20.0,3.7,1,4\n",
      "@data 0,1,0,1,0,0,1,0,0,0,1,9,1,4,0,0,1,8.33333333333,3.28571428571,3,4\n",
      "@data 0,1,0,0,0,0,0,0,0,0,0,11,1,1,0,0,0,7.66666666667,5.5,3,4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print arff_buf.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per-feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.',\n",
       " \"Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\"Meet/VB me/PRP today/NN at/IN the/DT FEC/NN in/IN DC/NN at/IN 4/NN ./.\", \"Wear/VB a/DT carnation/NN so/RB I/PRP know/VB it/PRP 's/POS you/PRP ./.\"]\n",
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentences(sentences):\n",
    "    tokens = [x.strip().split(' ') for x in sentences]\n",
    "    return [y.split('/') for x in tokens for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Meet', 'VB'],\n",
       " ['me', 'PRP'],\n",
       " ['today', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['FEC', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['DC', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['4', 'NN'],\n",
       " ['.', '.'],\n",
       " ['Wear', 'VB'],\n",
       " ['a', 'DT'],\n",
       " ['carnation', 'NN'],\n",
       " ['so', 'RB'],\n",
       " ['I', 'PRP'],\n",
       " ['know', 'VB'],\n",
       " ['it', 'PRP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['you', 'PRP'],\n",
       " ['.', '.']]"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['I', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    return [x[0].lower() in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['you', 'your', 'yours', 'u', 'ur', 'urs']\n",
    "    return [x[0].lower() in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def third_person_pronouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs']\n",
    "    return [x[0].lower() in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_person_pronouns(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coordinating_conjunctions(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['CC']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinating_conjunctions(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def past_tense_verbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['VBD']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense_verbs(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def future_tense_verbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [\"'ll\", 'will', 'gonna']\n",
    "    count = [x[0].lower() in candidate_words for x in token_split].count(True)\n",
    "    \n",
    "    # We also want to count sequences of going+to+VB\n",
    "    count += [token_split[i][0].lower() == 'going' and token_split[i + 1][0].lower() == 'to' and token_split[i + 2][1] == 'VB' for i in range(len(token_split) - 2)].count(True)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_tense_verbs(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commas(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [',']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commas(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colons(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = [':', ';']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colons(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dashes(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['-']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parantheses(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['(', ')']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ellipses(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['...']\n",
    "    return [x[0] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_nouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['NN', 'NNS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proper_nouns(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['NNP', 'NNPS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adverbs(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['RB', 'RBR', 'RBS']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wh_words(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['WDT', 'WP', 'WP$', 'WRB']\n",
    "    return [x[1] in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slang_acronyms(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['smh', 'fwb',  'lmfao', 'lmao', 'lms', 'tbh',  'rofl', 'wtf',\n",
    "                       'bff', 'wyd',  'lylc',  'brb',  'atm', 'imao', 'sml',  'btw',\n",
    "                       'bw',  'imho', 'fyi',   'ppl',  'sob', 'ttyl', 'imo',  'ltr',\n",
    "                       'thx', 'kk',   'omg',   'ttys', 'afn', 'bbs',  'cya',  'ez',\n",
    "                       'f2f', 'gtr',  'ic',    'jk',   'k',   'ly',   'ya',   'nm',  'np',\n",
    "                       'plz', 'ru',   'so',    'tc',   'tmi', 'ym',   'ur',   'u',   'sol']\n",
    "    return [x[0].lower() in candidate_words for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_acronyms(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upper_case_words(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    return [x[0].isupper() and len(x[0]) > 1 for x in token_split].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_case_words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_length(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    return len(token_split) / float(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.5"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_length(sentences):\n",
    "    token_split = split_sentences(sentences)\n",
    "    candidate_words = ['#', '$', '.', ',', ':', '(', ')', '\"', 'POS']\n",
    "    token_lengths = [len(x[0]) for x in token_split if x[1] not in candidate_words]\n",
    "    return sum(token_lengths) / float(len(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.888888888888889"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_sentences(sentences):\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying using WEKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Watson NLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"credentials\": {\n",
    "    \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api\",\n",
    "    \"username\": \"2bd0e6c7-5784-4967-860c-a9778754fdee\",\n",
    "    \"password\": \"rFs4Solusscl\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
